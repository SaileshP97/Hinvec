{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LingoIITGN/ganga-2-1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"token_per_language.json\", 'r') as f:\n",
    "    token_per_language = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitext Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: CrossSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_hindi = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/english-hindi_CrossSum/english-hindi_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        english_hindi.append(json.loads(sample))\n",
    "\n",
    "hindi_english = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/hindi-english_CrossSum/hindi-english_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        hindi_english.append(json.loads(sample))\n",
    "\n",
    "hindi_hindi = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/hindi-hindi_CrossSum/hindi-hindi_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        hindi_hindi.append(json.loads(sample))\n",
    "\n",
    "english_english = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/english-english_CrossSum/english-english_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        english_english.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "\n",
    "for sample in english_hindi:\n",
    "    eng = tokenizer.encode(sample['text'])\n",
    "    hin = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_english_tokens+= len(eng)\n",
    "    total_hindi_tokens+= len(hin)\n",
    "\n",
    "token_per_language['english_hindi_crosssum'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "\n",
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in hindi_english:\n",
    "    hin = tokenizer.encode(sample['text'])\n",
    "    eng = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_english_tokens+= len(eng)\n",
    "    total_hindi_tokens+= len(hin)\n",
    "\n",
    "token_per_language['hindi_english_crosssum'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "\n",
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in hindi_hindi:\n",
    "    hin_text = tokenizer.encode(sample['text'])\n",
    "    hin = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_text)\n",
    "    total_hindi_tokens+= len(hin)\n",
    "\n",
    "token_per_language['hindi_hindi_crosssum'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': 0,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "\n",
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in english_english:\n",
    "    english_text = tokenizer.encode(sample['text'])\n",
    "    english = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_english_tokens+= len(english_text)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['english_english_crosssum'] = {'Hindi': 0,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_url = []\n",
    "english_url = []\n",
    "\n",
    "hindi_url.extend([sample['target_url'] for sample in english_hindi])\n",
    "hindi_url.extend([sample['target_url'] for sample in hindi_hindi])\n",
    "hindi_url.extend([sample['source_url'] for sample in hindi_hindi])\n",
    "hindi_url.extend([sample['source_url'] for sample in hindi_english])\n",
    "\n",
    "english_url.extend([sample['source_url'] for sample in english_hindi])\n",
    "english_url.extend([sample['target_url'] for sample in english_english])\n",
    "english_url.extend([sample['source_url'] for sample in english_english])\n",
    "english_url.extend([sample['target_url'] for sample in hindi_english])\n",
    "\n",
    "hindi_url = set(hindi_url)\n",
    "english_url = set(english_url)\n",
    "\n",
    "hindi_url_dict = {url: f\"hi_{idx}\" for idx, url in enumerate(hindi_url)}\n",
    "english_url_dict = {url: f\"en_{idx}\" for idx, url in enumerate(english_url)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(english_hindi))\n",
    "print(len(hindi_english))\n",
    "print(len(hindi_hindi))\n",
    "print(len(english_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_english[317]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_hindi_hindi = []\n",
    "summary_instruction = \"निर्देश: दिए गए पाठ के लिए सबसे प्रासंगिक सारांश प्राप्त करें। पाठ:\"\n",
    "query_instruction = \"निर्देश: किसी दिए गए सारांश के लिए सबसे प्रासंगिक पैराग्राफ़ प्राप्त करें। सारांश: \"\n",
    "\n",
    "for idx, sample in enumerate(hindi_hindi):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_hindi_hindi_{hindi_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_hindi_hindi_{hindi_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_hindi_hindi.append(data)\n",
    "\n",
    "with open(\"Processed_data/crosssum_hindi_hindi_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_hindi_hindi:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_english[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_hindi_english = []\n",
    "\n",
    "summary_instruction = \"निर्देश: दिए गए पाठ के लिए सबसे प्रासंगिक सारांश प्राप्त करें। पाठ:\"\n",
    "query_instruction = \"Instructions: Retrieve the most relevant paragraph for a given summary. Summary: \"\n",
    "\n",
    "for idx, sample in enumerate(hindi_english):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_hindi_english_{english_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_hindi_english_{hindi_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_hindi_english.append(data)\n",
    "\n",
    "with open(f\"Processed_data/crosssum_hindi_english_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_hindi_english:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_hindi[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_english_hindi = []\n",
    "\n",
    "summary_instruction = \"Instructions: Retrieve the most relevant summary for the given paragraph. Text: \"\n",
    "query_instruction = \"निर्देश: किसी दिए गए सारांश के लिए सबसे प्रासंगिक पैराग्राफ़ प्राप्त करें। सारांश: \"\n",
    "\n",
    "for idx, sample in enumerate(english_hindi):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_english_hindi_{hindi_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_english_hindi_{english_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_english_hindi.append(data)\n",
    "\n",
    "with open(f\"Processed_data/crosssum_english_hindi_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_english_hindi:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample['text'])) for sample in english_hindi])\n",
    "print(f\"English article length: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['summary'])) for sample in english_hindi])\n",
    "print(f\"Hindi summary length: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['text'])) for sample in hindi_english])\n",
    "print(f\"Hindi article length: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['summary'])) for sample in hindi_english])\n",
    "print(f\"English summary length: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_english_english = []\n",
    "summary_instruction = \"Instructions: Retrieve the most relevant summary from a set of options for the given paragraph. Text: \"\n",
    "query_instruction = \"Instructions: Retrieve the most relevant paragraph from a set of options for a given summary. Summary: \"\n",
    "\n",
    "for idx, sample in enumerate(english_english):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_english_english_{english_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_english_english_{english_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_english_english.append(data)\n",
    "\n",
    "with open(\"Processed_data/crosssum_english_english_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_english_english:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Flores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Bitext Mining/Flores/flores_hi_en_test.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['examples'][230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in data['examples']:\n",
    "    hindi = tokenizer.encode(sample['source'])\n",
    "    english = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hindi)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['flores'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flores = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए हिंदी पाठ के लिए अर्थ की दृष्टि से सर्वाधिक समान अंग्रेजी पाठ प्राप्त करें। पाठ: \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar Hindi text for the given English text. Text: \"\n",
    "\n",
    "for idx, sample in enumerate(data['examples']):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data_ = {'id': f\"flores_{idx}\",\n",
    "                'source': hindi_instruction + sample['source'],\n",
    "                'target': sample['target']}\n",
    "    else:\n",
    "        data_ = {'id': f\"flores_{idx}\",\n",
    "                'source': english_instruction + sample['target'],\n",
    "                'target': sample['source']}\n",
    "\n",
    "    flores.append(data_)\n",
    "\n",
    "with open(f\"Processed_data/flores_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in flores:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_texts = np.array([])\n",
    "for sample in data['examples']:\n",
    "    len_of_texts = np.append(len_of_texts, len(tokenizer.encode(sample['source'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_texts.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = []\n",
    "with open(\"./Data/Bitext Mining/LASER/tatoeba.hin-eng.eng\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        english.append(sample)\n",
    "\n",
    "hindi = []\n",
    "with open(\"./Data/Bitext Mining/LASER/tatoeba.hin-eng.hin\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        hindi.append(sample)\n",
    "\n",
    "length = np.array([len(tokenizer.encode(sample)) for sample in english])\n",
    "print(f\"Average length of english sentences: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample)) for sample in hindi])\n",
    "print(f\"Average length of hindi sentences: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(hindi)):\n",
    "    hin = tokenizer.encode(hindi[idx])\n",
    "    eng = tokenizer.encode(english[idx])\n",
    "\n",
    "    total_hindi_tokens+= len(hin)\n",
    "    total_english_tokens+= len(eng)\n",
    "\n",
    "token_per_language['laser'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए हिंदी पाठ के लिए अर्थ की दृष्टि से सर्वाधिक समान अंग्रेजी पाठ प्राप्त करें। पाठ: \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar Hindi text for the given English text. Text: \"\n",
    "\n",
    "for idx, (hin, eng) in enumerate(zip(hindi, english)):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"laser_{idx}\",\n",
    "                'source': hindi_instruction + hin,\n",
    "                'target': eng}\n",
    "    else:\n",
    "        data = {'id': f\"laser_{idx}\",\n",
    "                'source': english_instruction + eng,\n",
    "                'target': hin}\n",
    "\n",
    "    laser.append(data)\n",
    "\n",
    "with open(f\"Processed_data/laser.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for sample in laser:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Mintaka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Bitext Mining/Mintaka/mintaka_test.json\", 'r') as f:\n",
    "    samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample['question'])) for sample in samples])\n",
    "print(f\"Average length of english questions: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['translations']['hi'])) for sample in samples])\n",
    "print(f\"Average length of hindi questions: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(samples)):\n",
    "    eng = tokenizer.encode(samples[idx]['question'])\n",
    "    hin = tokenizer.encode(samples[idx]['translations']['hi'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin)\n",
    "    total_english_tokens+= len(eng)\n",
    "\n",
    "token_per_language['Mintaka'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mintaka = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: हिंदी प्रश्न के लिए शब्दार्थ की दृष्टि से सर्वाधिक समान अंग्रेजी प्रश्न को पुनः प्राप्त करें। \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar Hindi question for the English question. Question: \"\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"mintaka_{idx}\",\n",
    "                'source': english_instruction + sample['question'],\n",
    "                'target': sample['translations']['hi']}\n",
    "    else:\n",
    "        data = {'id': f\"mintaka_{idx}\",\n",
    "                'source': hindi_instruction + sample['translations']['hi'],\n",
    "                'target': sample['question']}\n",
    "\n",
    "    mintaka.append(data)\n",
    "\n",
    "with open(f\"Processed_data/mintaka_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in mintaka:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: PHINC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/Bitext Mining/PHINC/filtered_data.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample)) for sample in df['Sentence']])\n",
    "print(f\"Average length of english questions: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample)) for sample in df['English_Translation']])\n",
    "print(f\"Average length of hindi questions: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[23,'Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[23, 'English_Translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "for sample in df.itertuples():\n",
    "    r_eng = tokenizer.encode(sample.Sentence)\n",
    "    eng = tokenizer.encode(sample.English_Translation)\n",
    "\n",
    "    total_r_english_tokens+= len(r_eng)\n",
    "    total_english_tokens+= len(eng)\n",
    "\n",
    "token_per_language['Mintaka'] = {'Hindi': 0,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phinc = []\n",
    "\n",
    "hindi_instruction = \"nirdesh: die gae romanakrt hindi paath ke lie arth kee drshti se sabase adhik samaan angrejee paath praapt karen. paath: \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar romanized Hindi text for the given English text. Text: \"\n",
    "\n",
    "for idx in range(len(df)):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"phinc_{idx}\",\n",
    "                'source': hindi_instruction + df.loc[idx, 'Sentence'],\n",
    "                'target': df.loc[idx, 'English_Translation']}\n",
    "    else:\n",
    "        data = {'id': f\"phinc_{idx}\",\n",
    "                'source': english_instruction + df.loc[idx, 'English_Translation'],\n",
    "                'target': df.loc[idx, 'Sentence']}\n",
    "\n",
    "    phinc.append(data)\n",
    "\n",
    "with open(f\"Processed_data/phinc.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in phinc:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: HindiDiscourseClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Classification/HindiDiscourseClassification/discourse_dataset.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(data[key]['Sentence'])) for key in data.keys()])\n",
    "print(f\"Average length of hindi text: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए हिंदी पाठ को निम्नलिखित में से किसी एक श्रेणी में वर्गीकृत करें: 'वर्णनात्मक', 'कथात्मक', 'संवाद', 'तर्कपूर्ण', 'सूचनात्मक', या 'अन्य'। पाठ: \"\n",
    "\n",
    "for key in data.keys():\n",
    "    label = data[key][\"Discourse Mode\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(data[key][\"Sentence\"])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        discourse.append({'id': f'discourse_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(discourse)\n",
    "        \n",
    "with open(f\"Processed_data/discourse.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in discourse:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in discourse:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['discourse'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"./Data/Classification/Massive/hi-IN.jsonl\", 'r') as f:\n",
    "    for sample in f:\n",
    "        data.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "massive = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए आदेश को निम्नलिखित में से किसी एक आशय श्रेणी में वर्गीकृत करें: 'अलार्म', 'ऑडियो', 'आईओटी', 'कैलेंडर', 'प्ले', 'सामान्य', 'डेटटाइम', 'टेकअवे', 'समाचार', 'संगीत', 'मौसम', 'क्यूए', 'सामाजिक', 'सिफारिश', 'खाना पकाना', 'परिवहन', 'ईमेल', 'सूचियाँ'। पाठ: \"\n",
    "\n",
    "for item in data:\n",
    "    label = item[\"scenario\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item[\"utt\"])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        massive.append({'id': f'massive_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(massive)\n",
    "        \n",
    "with open(f\"Processed_data/massive.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in massive:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [sample for sample in data if sample['partition']=='train']\n",
    "length = np.array([len(tokenizer.encode(sample['utt'])) for sample in train_data])\n",
    "print(f\"Average length of hindi text: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in massive:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['massive'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: SentimentAnalysisHindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"./Data/Classification/SentimentAnalysisHindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample['text'])) for sample in ds['train']])\n",
    "print(f\"Average length of hindi text: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: सकारात्मक, नकारात्मक, या तटस्थ। पाठ: \"\n",
    "\n",
    "for item in ds['train']:\n",
    "    label = item[\"label\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item[\"text\"])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Sentiment Analysis Joshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"./Data/Classification/sent_hineng_joshi/sentiment_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: नकारात्मक (-1), तटस्थ (0), या सकारात्मक (1)। पाठ:\"\n",
    "\n",
    "for (_,item) in data_csv.iterrows():\n",
    "    label = item.iloc[1]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item.iloc[0])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_joshi_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment_joshi.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_r_english_tokens+= len(hin_1)\n",
    "    total_r_english_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment_joshi'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Sentiment Shete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"./Data/Classification/sent_hineng_shete/data.csv\")\n",
    "print(len(data_csv))\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: नकारात्मक (-1), तटस्थ (0), या सकारात्मक (1)। पाठ:\"\n",
    "label_dict = {-1: 'neg',\n",
    "              0: 'neu',\n",
    "              1: 'pos'}\n",
    "for (_,item) in data_csv.iterrows():\n",
    "    label = label_dict[item.iloc[1]]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item.iloc[0])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_shete_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment_shete.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_r_english_tokens+= len(hin_1)\n",
    "    total_r_english_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment_shete'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Sentiment Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"Data/Classification/sent_review/sentiment_reviews.csv\")\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: नकारात्मक (-1), या सकारात्मक (1)। पाठ:\"\n",
    "\n",
    "for (_,item) in data_csv.iterrows():\n",
    "    label = item.iloc[1]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item.iloc[0])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_review_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment_review.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment_review'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Amazon Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([sample['label'] for sample in ds['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"Instruction: Classify the sentiment of the following Amazon product review into one of the following labels:\\n0 - Very Negative  \\n1 - Negative \\n2 - Neutral  \\n3 - Positive  \\n4 - Very Positive \\nReview: \"\n",
    "\n",
    "for item in ds['train']:\n",
    "    label = item['label']\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item['text'])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        amazon_review.append({'id': f'amazon_review_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(amazon_review)\n",
    "        \n",
    "with open(f\"Processed_data/amazon_review.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in amazon_review:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review_test = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"Instruction: Classify the sentiment of the following Amazon product review into one of the following labels:\\n0 - Very Negative  \\n1 - Negative \\n2 - Neutral  \\n3 - Positive  \\n4 - Very Positive \\nReview: \"\n",
    "\n",
    "for item in ds['validation']:\n",
    "    label = item['label']\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item['text'])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        amazon_review_test.append({'id': f'amazon_review_val{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(amazon_review_test)\n",
    "        \n",
    "with open(f\"Processed_data/amazon_review_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in amazon_review_test:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in amazon_review:\n",
    "    eng_1 = tokenizer.encode(sample['source'])\n",
    "    eng_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_english_tokens+= len(eng_1)\n",
    "    total_english_tokens+= len(eng_2)\n",
    "\n",
    "token_per_language['amazon_review'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ABP news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Data/Classification/ABP_News/ABP_News_classification.json\", \"r\") as f:\n",
    "    abp_news = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp_news_classification = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: निम्नलिखित समाचार लेख को दिए गए श्रेणियों में से किसी एक में वर्गीकृत करें: श्रेणियाँ: gk, technology, business, entertainment, agriculture, astro, lifestyle, sports, education, states. समाचार लेख:\"\n",
    "\n",
    "for key, val in abp_news.items():\n",
    "    label = val['domain']\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(val['article'])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        abp_news_classification.append({'id': f'abp_news_classification_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(abp_news_classification)\n",
    "        \n",
    "with open(f\"Processed_data/abp_news_classification.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in abp_news_classification:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp_news_classification[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in abp_news_classification:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['abp_news_classification'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: MTOP Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mteb.tasks import MTOPIntentClassification\n",
    "\n",
    "task = MTOPIntentClassification()\n",
    "task.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.dataset['hi']['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "intent = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को उसके उद्देश्य के आधार पर वर्गीकृत करें। पाठ: \"\n",
    "\n",
    "for item in task.dataset['hi']['train']:\n",
    "    label = item[\"label\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item[\"text\"])\n",
    "\n",
    "drop_key = []\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    if len(label_groups[key]) < 2:\n",
    "        drop_key.append(key)\n",
    "    \n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    if key not in drop_key:\n",
    "\n",
    "        for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "            pos_idx = list(range(0, len(label_groups[key])))\n",
    "            pos_idx.remove(idx)\n",
    "            intent.append({'id': f'intent_{key}',\n",
    "                            'source': instruction + sent,\n",
    "                            'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(intent)\n",
    "        \n",
    "with open(f\"Processed_data/mtop_intent.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in intent:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in intent:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['intent'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"mteb/xnli\", \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "xnli = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए प्रेज़म और हाइपोथेसिस के आधार पर निर्धारित करें कि संबंध 'अनुकूलन (entailment)', 'तटस्थ (neutral)', या 'विरोधाभासी (contradiction)' है। \"\n",
    "\n",
    "for idx in range(392702):\n",
    "    label = ds['train'][idx][\"label\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(f\"आधार: {ds['train'][idx]['premise']} परिकल्पना: {ds['train'][idx]['hypothesis']} \")\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        xnli.append({'id': f'xnli_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(xnli)\n",
    "        \n",
    "with open(f\"Processed_data/xnli.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in xnli:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in xnli:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['xnli'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ai4bharat/samanantar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = {}\n",
    "\n",
    "for language in ['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']:\n",
    "    data[language] = load_dataset(\"ai4bharat/samanantar\", language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_as = load_dataset(\"ai4bharat/samanantar\", \"as\")\n",
    "ds_bn = load_dataset(\"ai4bharat/samanantar\", \"bn\")\n",
    "ds_gu = load_dataset(\"ai4bharat/samanantar\", \"gu\")\n",
    "ds_hi = load_dataset(\"ai4bharat/samanantar\", \"hi\")\n",
    "ds_kn = load_dataset(\"ai4bharat/samanantar\", \"kn\")\n",
    "ds_ml = load_dataset(\"ai4bharat/samanantar\", \"ml\")\n",
    "ds_mr = load_dataset(\"ai4bharat/samanantar\", \"mr\")\n",
    "ds_or = load_dataset(\"ai4bharat/samanantar\", \"or\")\n",
    "ds_pa = load_dataset(\"ai4bharat/samanantar\", \"pa\")\n",
    "ds_ta = load_dataset(\"ai4bharat/samanantar\", \"ta\")\n",
    "ds_te = load_dataset(\"ai4bharat/samanantar\", \"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['as']['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "language_classification = []\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ की भाषा को निम्नलिखित भाषाओं में से किसी एक के रूप में वर्गीकृत करें: असमिया (as), बांग्ला (bn), गुजराती (gu), हिंदी (hi), कन्नड़ (kn), मलयालम (ml), मराठी (mr), उड़िया (or), पंजाबी (pa), तमिल (ta), या तेलुगू (te)। पाठ: \"\n",
    "\n",
    "for key in ['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']:\n",
    "\n",
    "    for idx, sent in enumerate(data[key]['train']):\n",
    "\n",
    "        if key=='hi' and idx>5000:\n",
    "            break\n",
    "        elif key!='hi' and idx>500:\n",
    "            break\n",
    "\n",
    "        language_classification.append({'id': f'samanantar_{key}',\n",
    "                        'source': instruction + sent['tgt'],\n",
    "                        'target': data[key]['train'][data[key]['train'].num_rows-1-idx]['tgt']})\n",
    "        \n",
    "random.shuffle(language_classification)\n",
    "        \n",
    "with open(f\"Processed_data/samanantar_language_classification.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in language_classification:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Code Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"Data/Translation/codemixed_parallel_corpus/English-Hindi code-mixed parallel corpus.csv\")\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in data_csv.itertuples():\n",
    "    hin_r = tokenizer.encode(sample.Sentence)\n",
    "    english = tokenizer.encode(sample.English_Translation)\n",
    "\n",
    "    total_r_english_tokens+= len(hin_r)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['code_mixed'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mixed = []\n",
    "hindi_instruction = \"Nirdesh: Diye gaye Hindi vaakya se sabse saman romanised hindi vaakya dhunde. Vaakya: \"\n",
    "english_instruction = \"Instruction: Find the most similar hindi sentence to the given romanised hindi sentence. Sentence: \"\n",
    "\n",
    "def remove_username(sentence):\n",
    "\n",
    "    return \" \".join([word for word in sentence.split() if word[0]!='@'])\n",
    "\n",
    "for (idx, sample) in data_csv.iterrows():\n",
    "\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"code_mixed_{idx}\",\n",
    "                'source': hindi_instruction + remove_username(sample.iloc[0]),\n",
    "                'target': remove_username(sample.iloc[1])}\n",
    "    else:\n",
    "        data = {'id': f\"code_mixed_{idx}\",\n",
    "                'source': english_instruction + remove_username(sample.iloc[1]),\n",
    "                'target': remove_username(sample.iloc[0])}\n",
    "\n",
    "\n",
    "    code_mixed.append(data)\n",
    "\n",
    "with open(\"Processed_data/code_mixed.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in code_mixed:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: HinGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data_csv = pd.read_csv(\"Data/Translation/HinGE/HinGE.csv\")\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in data_csv.itertuples():\n",
    "    hindi = tokenizer.encode(sample.Hindi)\n",
    "    english = tokenizer.encode(sample.English)\n",
    "\n",
    "    total_hindi_tokens+= len(hindi)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['hinge'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinge = []\n",
    "englishr_instruction = \"Instruction: From the given English sentence, find the translated romanised Hindi sentence. Sentence: \"\n",
    "hindi_instruction = \"निर्देश: दिए गए हिंदी वाक्य में से अनुवादित रोमनकृत हिंदी वाक्य चुनिए। वाक्य: \"\n",
    "hindir_instruction = \"Nirdesh: Diye gaye romanised hindi vaakya se sabse saman hindi vaakya dhunde. Vaakya: \"\n",
    "hindire_instruction = \"Nirdesh: Diye gaye romanised hindi vaakya se sabse saman english vaakya dhunde. Vaakya: \"\n",
    "\n",
    "\n",
    "for (idx, sample) in data_csv.iterrows():\n",
    "\n",
    "    if len(re.findall(r\"'(.*?)'\", sample.iloc[2]))==0:\n",
    "        continue\n",
    "    if idx%2==0:\n",
    "        data1 = {'id': f\"hinge_{idx}\",\n",
    "                'source': hindi_instruction + sample.iloc[1],\n",
    "                'target': re.findall(r\"'(.*?)'\", sample.iloc[2])[0]}\n",
    "        hinge.append(data1)\n",
    "        data2 = {'id': f\"hinge_{idx}\",\n",
    "                'source': hindir_instruction + re.findall(r\"'(.*?)'\", sample.iloc[2])[0],\n",
    "                'target': sample.iloc[1]}\n",
    "        hinge.append(data2)\n",
    "\n",
    "    else:\n",
    "        data1 = {'id': f\"hinge_{idx}\",\n",
    "                'source': hindire_instruction + re.findall(r\"'(.*?)'\", sample.iloc[2])[0],\n",
    "                'target': sample.iloc[1]}\n",
    "        hinge.append(data1)\n",
    "        data2 = {'id': f\"hinge_{idx}\",\n",
    "                'source': englishr_instruction + sample.iloc[0],\n",
    "                'target': re.findall(r\"'(.*?)'\", sample.iloc[2])[0]}\n",
    "        hinge.append(data2)\n",
    "    \n",
    "\n",
    "\n",
    "    hinge.append(data)\n",
    "\n",
    "with open(\"Processed_data/hinge.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in hinge:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hinge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: IndicQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Retrieval/IndicQA/indicqa.hi.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = set([sample['paragraphs'][0]['context'] for sample in data['data']])\n",
    "context_dict = {context: idx for idx, context in enumerate(context_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][34]['paragraphs'][0]['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicqa = []\n",
    "\n",
    "instruction = \"निर्देश: दिए गए प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक गद्यांश चुनिए। प्रश्न: \"\n",
    "\n",
    "for sample in data['data']:\n",
    "\n",
    "    \n",
    "    context = sample['paragraphs'][0]['context']\n",
    "\n",
    "    for qas in sample['paragraphs'][0]['qas']:\n",
    "\n",
    "        if qas['category'] == 'No':\n",
    "            continue\n",
    "        indicqa.append({\n",
    "            'id': f\"indicqa_{context_dict[context]}\",\n",
    "            'source': instruction + qas['question'],\n",
    "            'target': context\n",
    "        })\n",
    "\n",
    "random.shuffle(indicqa)\n",
    "\n",
    "with open(f\"Processed_data/indicqa.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in indicqa:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in indicqa:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['indicqa'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: MLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"./Data/Retrieval/MLDR/test.jsonl\", \"r\") as f:\n",
    "\n",
    "    for sample in f:\n",
    "        data.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_passage = []\n",
    "negative_passage = []\n",
    "\n",
    "for sample in data:\n",
    "    for pp in sample['positive_passages']:\n",
    "        positive_passage.append(pp['text'])\n",
    "    for np in sample['negative_passages']:\n",
    "        negative_passage.append(np['text'])\n",
    "\n",
    "positive_passage = set(positive_passage)\n",
    "negative_passage = set(negative_passage)\n",
    "\n",
    "pp_dict = {passage: idx for idx, passage in enumerate(positive_passage)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldr = []\n",
    "\n",
    "instruction = \"निर्देश: दिए गए प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक अनुच्छेद को चुनें। प्रश्न: \"\n",
    "\n",
    "for sample in data:\n",
    "\n",
    "    passage = sample['positive_passages'][0]['text']\n",
    "    query = sample['query']\n",
    "\n",
    "    mldr.append({\n",
    "        'id': f\"mldir_{pp_dict[passage]}\",\n",
    "        'source': instruction + query,\n",
    "        'target': passage\n",
    "    })\n",
    "\n",
    "random.shuffle(mldr)\n",
    "\n",
    "with open(f\"Processed_data/mldr_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in mldr:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in mldr:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['mldr'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: MLQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Retrieval/MLQA_V1/test/test-context-en-question-hi.json\", \"r\") as f:\n",
    "    hin_eng_data = json.load(f)\n",
    "with open(\"./Data/Retrieval/MLQA_V1/test/test-context-hi-question-en.json\", \"r\") as f:\n",
    "    eng_hin_data = json.load(f)\n",
    "with open(\"./Data/Retrieval/MLQA_V1/test/test-context-hi-question-hi.json\", \"r\") as f:\n",
    "    hin_hin_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_context = []\n",
    "english_context = []\n",
    "\n",
    "for topic in hin_hin_data['data']:\n",
    "    \n",
    "    for sample in topic['paragraphs']:\n",
    "        hindi_context.append(sample['context'])\n",
    "\n",
    "hindi_context = set(hindi_context)\n",
    "\n",
    "hindi_context_dict = {passage: idx for idx, passage in enumerate(hindi_context)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlqa = []\n",
    "\n",
    "instruction = \"निर्देश: प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक संदर्भ प्राप्त करें। प्रश्न:\"\n",
    "\n",
    "for topic in hin_hin_data['data']:\n",
    "\n",
    "    for sample in topic['paragraphs']:\n",
    "\n",
    "        context = sample['context']\n",
    "\n",
    "        for qas in sample['qas']:\n",
    "            mlqa.append({\n",
    "                'id': f\"mlqa_{hindi_context_dict[context]}\",\n",
    "                'source': instruction + qas['question'],\n",
    "                'target': context               \n",
    "            })\n",
    "\n",
    "random.shuffle(mlqa)\n",
    "with open(\"./Processed_data/mlqa_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in mlqa:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in mlqa:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['mlqa'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ABP news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Data/Retrieval/ABP_news/ABP_new_query_doc.json\", \"r\") as f:\n",
    "    query_doc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp = []\n",
    "\n",
    "instruction = \"निर्देश: प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक समाचार लेख ढूंढें। प्रश्न:\"\n",
    "count = 0\n",
    "\n",
    "for field in query_doc.keys():\n",
    "\n",
    "    for idx, sample in enumerate(query_doc[field]):\n",
    "\n",
    "        context = sample[0]\n",
    "        question = sample[1]\n",
    "\n",
    "        if question is None:\n",
    "            count+=1\n",
    "            continue\n",
    "\n",
    "        abp.append({\n",
    "            'id': f\"abp_{field}_{idx}\",\n",
    "            'source': instruction + question,\n",
    "            'target': context          \n",
    "        })\n",
    "\n",
    "random.shuffle(abp)\n",
    "with open(\"./Processed_data/abp_news.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in abp:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in abp:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['abp'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(ds['train'])):\n",
    "    context = tokenizer.encode(ds['train'][idx]['context'])\n",
    "    qs = tokenizer.encode(ds['train'][idx]['question'])\n",
    "\n",
    "    total_english_tokens+= len(context)\n",
    "    total_english_tokens+= len(qs)\n",
    "\n",
    "token_per_language['squad'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = []\n",
    "\n",
    "instruction = \"Instruction: Given a question, retrieve the most relevant passage. Question: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    context = ds['train'][idx]['context']\n",
    "    question = ds['train'][idx]['question']\n",
    "\n",
    "    squad.append({\n",
    "        'id': f\"squad_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': context          \n",
    "    })\n",
    "\n",
    "random.shuffle(squad)\n",
    "with open(\"./Processed_data/squad.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in squad:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = []\n",
    "\n",
    "instruction = \"Instruction: Given a question, retrieve the most relevant passage. Question: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['validation'])):\n",
    "\n",
    "    context = ds['validation'][idx]['context']\n",
    "    question = ds['validation'][idx]['question']\n",
    "\n",
    "    squad.append({\n",
    "        'id': f\"squad_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': context          \n",
    "    })\n",
    "\n",
    "random.shuffle(squad)\n",
    "with open(\"./Processed_data/squad_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in squad:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sentence-transformers/eli5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(ds['train'])):\n",
    "    answer = tokenizer.encode(ds['train'][idx]['answer'])\n",
    "    qs = tokenizer.encode(ds['train'][idx]['question'])\n",
    "\n",
    "    total_english_tokens+= len(answer)\n",
    "    total_english_tokens+= len(qs)\n",
    "\n",
    "token_per_language['eli5'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = []\n",
    "\n",
    "instruction = \"Instruction: Given a question, retrieve the most relevant answer. Question: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    answer = ds['train'][idx]['answer']\n",
    "    question = ds['train'][idx]['question']\n",
    "\n",
    "    eli5.append({\n",
    "        'id': f\"eli5_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(eli5)\n",
    "with open(\"./Processed_data/eli5.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in eli5:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Stackover flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/stackoverflowdupquestions-reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in ds['train']:\n",
    "    query = tokenizer.encode(sample['query'])\n",
    "    positive = tokenizer.encode(sample['positive'][0])\n",
    "\n",
    "    total_english_tokens+= len(query)\n",
    "    total_english_tokens+= len(positive)\n",
    "\n",
    "token_per_language['stackoverflow'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow = []\n",
    "\n",
    "instruction = \"Instruction: Given a query, retrieve the most similar sentence. Query: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    answer = ds['train'][idx]['positive'][0]\n",
    "    question = ds['train'][idx]['query']\n",
    "\n",
    "    stackoverflow.append({\n",
    "        'id': f\"stackoverflow_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(stackoverflow)\n",
    "with open(\"./Processed_data/stackoverflow.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in stackoverflow:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_test = []\n",
    "\n",
    "instruction = \"Instruction: Given a query, retrieve the most similar sentence. Query: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    answer = ds['train'][idx]['positive'][0]\n",
    "    question = ds['train'][idx]['query']\n",
    "\n",
    "    stackoverflow_test.append({\n",
    "        'id': f\"stackoverflow_test_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(stackoverflow_test)\n",
    "with open(\"./Processed_data/stackoverflow_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in stackoverflow_test:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"token_per_language.json\", 'w') as f:\n",
    "    json.dump(token_per_language, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTEB datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: XNLI New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/xnli\", \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "xnli_new = []\n",
    "\n",
    "instruction = \"Instruction: Given a query, retrieve the most similar sentence. Query: \"\n",
    "count = 0\n",
    "\n",
    "for idx, sample in enumerate(ds['train']):\n",
    "\n",
    "    if sample['label'] != 0:\n",
    "        continue\n",
    "\n",
    "    answer = sample['premise']\n",
    "    question = sample['hypothesis']\n",
    "\n",
    "    xnli_new.append({\n",
    "        'id': f\"xnli_new_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(xnli_new)\n",
    "with open(\"./Processed_data/xnli_new.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in xnli_new:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Belebele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_dev = load_dataset(\"facebook/belebele\", \"hin_Deva\")\n",
    "ds_latin = load_dataset(\"facebook/belebele\", \"hin_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_latin['test'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "belebele = []\n",
    "\n",
    "eng_instruction = \"Instruction: Given a query, retrieve the most similar passage. Query: \"\n",
    "hindi_instruction = \"निर्देश: एक प्रश्न दिया गया है, सबसे समान अनुच्छेद को पुनः प्राप्त करें। प्रश्न: \"\n",
    "count = 0\n",
    "\n",
    "for idx, sample in enumerate(ds_latin['test']):\n",
    "\n",
    "    passage = sample['flores_passage']\n",
    "    question = sample['question']\n",
    "\n",
    "    belebele.append({\n",
    "        'id': f\"belebele_{idx}\",\n",
    "        'source': eng_instruction + question,\n",
    "        'target': passage          \n",
    "    })\n",
    "\n",
    "max_idx = idx\n",
    "\n",
    "for idx, sample in enumerate(ds_dev['test']):\n",
    "\n",
    "    passage = sample['flores_passage']\n",
    "    question = sample['question']\n",
    "\n",
    "    belebele.append({\n",
    "        'id': f\"belebele_{max_idx + idx+1}\",\n",
    "        'source': hindi_instruction + question,\n",
    "        'target': passage          \n",
    "    })\n",
    "\n",
    "random.shuffle(belebele)\n",
    "with open(\"./Processed_data/belebele.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in belebele:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: IN22-Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mteb.tasks import IN22ConvBitextMining\n",
    "\n",
    "task = IN22ConvBitextMining()\n",
    "task.load_data()\n",
    "task.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ai4bharat/IN22-Conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "convbtm = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए पाठ का सबसे समान अनुवाद खोजें। पाठ: \"\n",
    "\n",
    "for idx, sample in enumerate(ds['test']):\n",
    "\n",
    "    for taregt_lang in ['asm_Beng', 'ben_Beng', 'brx_Deva', 'doi_Deva', 'eng_Latn', 'gom_Deva', 'guj_Gujr', 'kan_Knda', 'kas_Arab', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'mni_Mtei', 'npi_Deva', 'ory_Orya', 'pan_Guru', 'san_Deva', 'sat_Olck', 'snd_Deva', 'tam_Taml', 'tel_Telu', 'urd_Arab']:\n",
    "\n",
    "        text = sample['hin_Deva']\n",
    "        target = sample[taregt_lang]\n",
    "\n",
    "        convbtm.append({\n",
    "            'id': f\"convbtm_{idx}\",\n",
    "            'source': hindi_instruction + text,\n",
    "            'target': target          \n",
    "        })\n",
    "\n",
    "random.shuffle(convbtm)\n",
    "with open(\"./Processed_data/convbtm.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in convbtm:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: LinceMTBitextMining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/LinceMTBitextMining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "lince = []\n",
    "\n",
    "instruction = \"Instruction: Find the most similar romanised Hindi sentence of the give english sentence. Sentence: \"\n",
    "\n",
    "for idx, sample in enumerate(ds['test']):\n",
    "\n",
    "\n",
    "    sent1 = sample['hin_Deva']\n",
    "    sent2 = sample[taregt_lang]\n",
    "\n",
    "    lince.append({\n",
    "        'id': f\"lince_{idx}\",\n",
    "        'source': instruction + sent1,\n",
    "        'target': sent2          \n",
    "    })\n",
    "\n",
    "random.shuffle(lince)\n",
    "with open(\"./Processed_data/lince.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in lince:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: WikiReranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ellamind/wikipedia-2023-11-reranking-multilingual\", \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "wikireranking = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए प्रश्न के लिए सबसे अधिक समानता रखने वाले दस्तावेज़ को पहचानें और पुनः प्राप्त करें। प्रश्न:\"\n",
    "\n",
    "for idx, sample in enumerate(ds['test']):\n",
    "\n",
    "    query = sample['query']\n",
    "    positive = sample['positive'][0]\n",
    "\n",
    "    wikireranking.append({\n",
    "        'id': f\"wikireranking_{idx}\",\n",
    "        'source': hindi_instruction + query,\n",
    "        'target': positive          \n",
    "    })\n",
    "\n",
    "random.shuffle(wikireranking)\n",
    "with open(\"./Processed_data/wikireranking.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in wikireranking:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: IndicCrosslingualSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mteb.tasks import IndicCrosslingualSTS\n",
    "\n",
    "task = IndicCrosslingualSTS()\n",
    "task.load_data()\n",
    "task.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hindi = 0\n",
    "English = 0\n",
    "Romanised_Hindi = 0\n",
    "\n",
    "for key in token_per_language.keys():\n",
    "\n",
    "    Hindi+=token_per_language[key]['Hindi']\n",
    "    English+=token_per_language[key]['English']\n",
    "    Romanised_Hindi+=token_per_language[key]['Romanised_Hindi']\n",
    "\n",
    "print(Hindi)\n",
    "print(English)\n",
    "print(Romanised_Hindi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Length Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"./Processed_data/*\")\n",
    "\n",
    "data = []\n",
    "token_len = []\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "\n",
    "        for sample in f:\n",
    "            sample = json.loads(sample)\n",
    "            data.append(sample)\n",
    "\n",
    "            source_tok = len(tokenizer(sample['source']).input_ids)\n",
    "            target_tok = len(tokenizer(sample['target']).input_ids)\n",
    "\n",
    "            token_len.append(source_tok)\n",
    "            token_len.append(target_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "token_len = np.array(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = sorted(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(token_len, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import glob, json, os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "files = glob.glob(\"./Processed_data/*\")\n",
    "\n",
    "def get_jsonl(file):\n",
    "    data = []\n",
    "    with open(file, 'r') as f:\n",
    "        for sample in f:\n",
    "            data.append(json.loads(sample))\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for sample in data:\n",
    "            json.dump(sample, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    if \"_test\" in file:\n",
    "        continue\n",
    "\n",
    "    if file not in [\"./Processed_data/crosssum_english_hindi.jsonl\", \"./Processed_data/crosssum_hindi_english.jsonl\",\n",
    "                    \"./Processed_data/crosssum_hindi_hindi.jsonl\", \"./Processed_data/crosssum_english_english.jsonl\",\n",
    "                    \"./Processed_data/flores.jsonl\", \"./Processed_data/mintaka.jsonl\",\n",
    "                    \"./Processed_data/mldr.jsonl\", \"./Processed_data/mlqa.jsonl\",\n",
    "                    \"./Processed_data/amazon_review.jsonl\", \"./Processed_data/squad.jsonl\",\n",
    "                    \"./Processed_data/stackoverflow.jsonl\"]:\n",
    "        \n",
    "        data = get_jsonl(file)\n",
    "        random.shuffle(data)\n",
    "        train, val = train_test_split(data, test_size=0.2)\n",
    "\n",
    "        path = f\"./training_data/{file.split('/')[-1].split('.')[0]}/\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        save_jsonl(train, f\"{path}train.jsonl\")\n",
    "        save_jsonl(val, f\"{path}val.jsonl\")\n",
    "    else:\n",
    "        data = get_jsonl(file)\n",
    "        path = f\"./training_data/{file.split('/')[-1].split('.')[0]}/\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        save_jsonl(data, f\"{path}train.jsonl\")\n",
    "\n",
    "        test_path = f\".{file.split('.')[1]}_test.{file.split('.')[2]}\"\n",
    "        data = get_jsonl(test_path)\n",
    "        save_jsonl(data, f\"{path}val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "train_files = glob.glob(\"./training_data/*/train.jsonl\", recursive=True)\n",
    "val_files = glob.glob(\"./training_data/*/val.jsonl\", recursive=True)\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "exclude_files = ['sentiment_shete', 'sentiment_joshi', 'hinge', 'code_mixed', 'sentiment_review', 'abp_news', 'crosssum_english_english']\n",
    "exclude_files = ['xnli', 'Wikireranking', 'samanantar_language_classification']\n",
    "#exclude_files = ['samanantar_language_classification']\n",
    "\n",
    "english_data_files = ['amazon_review', 'crosssum_english_english', 'eli5', 'squad']\n",
    "#english_data_files = []\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for sample in data:\n",
    "            json.dump(sample, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "for file in train_files:\n",
    "\n",
    "    if file.split('/')[2] in exclude_files:\n",
    "        #print(file)\n",
    "        continue\n",
    "    print(file)\n",
    "\n",
    "    if file.split('/')[2] in english_data_files:\n",
    "        data = []\n",
    "        with open(file, 'r') as f:\n",
    "            for sample in f:\n",
    "                data.append(json.loads(sample))\n",
    "        length_of_data = len(data)\n",
    "        train_data = train_data + data[:length_of_data//2]\n",
    "\n",
    "    else:\n",
    "\n",
    "        with open(file, 'r') as f:\n",
    "            for sample in f:\n",
    "                train_data.append(json.loads(sample))\n",
    "\n",
    "for file in val_files:\n",
    "\n",
    "    if file.split('/')[2] in exclude_files:\n",
    "        continue\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for sample in f:\n",
    "            val_data.append(json.loads(sample))\n",
    "\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "\n",
    "save_jsonl(train_data, \"./new_training_data/train_data.jsonl\")\n",
    "save_jsonl(val_data, \"./new_training_data/val_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "659297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Hard Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "def generate_query_for_article(sample):\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=\"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\"\n",
    "    )\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "        You are an AI assistant designed to generate challenging hard negative examples in the same language as the output. Your task is to produce exactly one concise and well-formed hard negative response that seems similar to the correct Output text, but is actually irrelevant for the given Input text. The hard negative should be misleading in a subtle way — close in topic or style, but not a valid answer. Make sure the grammar and vocabulary are correct. Wrap the hard negative inside ## markers like this: ## hard negative text ##*.**\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": f\"Input Text: {sample['source']} \\nOutput Text: {sample['target']} \\nGenerate Hard Negative example: \"}\n",
    "            ],\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            max_tokens=100,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stream=False\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        time.sleep(5)\n",
    "        return generate_query_for_article(sample)\n",
    "    \n",
    "    return completion.choices[0].message.content.split(\"##\")[1]\n",
    "\n",
    "with open(\"./new_training_data/train_data.jsonl\", \"r\") as f:\n",
    "    data = []\n",
    "    for sample in f:\n",
    "        data.append(json.loads(sample))\n",
    "\n",
    "for idx, sample in enumerate(data):\n",
    "    \n",
    "    data[idx]['hard_negative'] = generate_query_for_article(sample)\n",
    "    data[idx]['hard_negative_flag'] = 1\n",
    "\n",
    "    if idx%5000==0:\n",
    "        print(f\"Processed {idx} samples\")\n",
    "        with open(\"./new_training_data/train_data_with_hard_negative.jsonl\", \"w\") as f:\n",
    "            for sample in data:\n",
    "                json.dump(sample, f, ensure_ascii=False)\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LingoIITGN/Ganga-2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bitext_Mining = ['./training_data/crosssum_english_english/train.jsonl',\n",
    "                 './training_data/crosssum_hindi_hindi/train.jsonl',\n",
    "                 './training_data/crosssum_english_hindi/train.jsonl',\n",
    "                 './training_data/crosssum_hindi_english/train.jsonl',\n",
    "                 './training_data/flores/train.jsonl',\n",
    "                 './training_data/laser/train.jsonl',\n",
    "                 './training_data/mintaka/train.jsonl',\n",
    "                 './training_data/phinc/train.jsonl']\n",
    "\n",
    "Classification = ['./training_data/discourse/train.jsonl',\n",
    "                  './training_data/massive/train.jsonl',\n",
    "                  './training_data/sentiment_joshi/train.jsonl',\n",
    "                  './training_data/sentiment_shete/train.jsonl',\n",
    "                  './training_data/sentiment_review/train.jsonl',\n",
    "                  './training_data/sentiment/train.jsonl',\n",
    "                 './training_data/abp_news_classification/train.jsonl',\n",
    "                 './training_data/amazon_review/train.jsonl'\n",
    "                  ]\n",
    "\n",
    "Retrieval = ['./training_data/abp_news/train.jsonl',\n",
    "             './training_data/indicqa/train.jsonl',\n",
    "             './training_data/mldr/train.jsonl',\n",
    "             './training_data/mlqa/train.jsonl',\n",
    "             './training_data/squad/train.jsonl',\n",
    "             './training_data/stackoverflow/train.jsonl',\n",
    "             './training_data/eli5/train.jsonl'\n",
    "            ]\n",
    "\n",
    "Translation = ['./training_data/code_mixed/train.jsonl',\n",
    "               './training_data/hinge/train.jsonl',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {}\n",
    "\n",
    "for task in Bitext_Mining:\n",
    "    task_dict[task] = \"Bitext_Mining\"\n",
    "\n",
    "for task in Classification:\n",
    "    task_dict[task] = \"Classification\"\n",
    "\n",
    "for task in Retrieval:\n",
    "    task_dict[task] = \"Retrieval\"\n",
    "\n",
    "for task in Translation:\n",
    "    task_dict[task] = \"Translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_task = {}\n",
    "for task_type, files in task_dict.items():\n",
    "    for fname in files:\n",
    "        file_to_task[fname] = task_type\n",
    "\n",
    "# Collect all .jsonl files\n",
    "files = glob.glob(\"./training_data/*/train.jsonl\", recursive=True)\n",
    "\n",
    "summary = {\n",
    "    \"overall_total_tokens\": 0,\n",
    "    \"overall_samples\": 0,\n",
    "    \"task_type_summary\": defaultdict(lambda: {\n",
    "        \"total_tokens\": 0,\n",
    "        \"total_samples\": 0,\n",
    "        \"max_token_length\": 0,\n",
    "        \"min_token_length\": float(\"inf\")\n",
    "    }),\n",
    "    \"file_summary\": {}\n",
    "}\n",
    "\n",
    "for file in files:\n",
    "    num_tokens = 0\n",
    "    num_sample = 0\n",
    "    max_length = 0\n",
    "    min_length = float(\"inf\")\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            source = sample['source']\n",
    "            target = sample['target']\n",
    "            token_len = len(tokenizer.encode(source)) + len(tokenizer.encode(target))\n",
    "            \n",
    "            num_tokens += token_len\n",
    "            num_sample += 1\n",
    "            max_length = max(max_length, token_len)\n",
    "            min_length = min(min_length, token_len)\n",
    "        \n",
    "    #filename = os.path.basename(file)\n",
    "    task_type = task_dict.get(file, \"unknown\")\n",
    "\n",
    "    # Update task type summary\n",
    "    task_data = summary[\"task_type_summary\"][task_type]\n",
    "    task_data[\"total_tokens\"] += num_tokens\n",
    "    task_data[\"total_samples\"] += num_sample\n",
    "    task_data[\"max_token_length\"] = max(task_data[\"max_token_length\"], max_length)\n",
    "    task_data[\"min_token_length\"] = min(task_data[\"min_token_length\"], min_length)\n",
    "\n",
    "    summary['file_summary'][file] = {\n",
    "                        \"task_type\": task_type,\n",
    "                        \"total_samples\": num_sample,\n",
    "                        \"total_tokens\": num_tokens,\n",
    "                        \"max_token_length\": max_length,\n",
    "                        \"min_token_length\": min_length,\n",
    "    }\n",
    "\n",
    "    summary[\"overall_total_tokens\"] += num_tokens\n",
    "    summary[\"overall_samples\"] += num_sample\n",
    "\n",
    "# Convert defaultdict to dict for saving\n",
    "summary[\"task_type_summary\"] = dict(summary[\"task_type_summary\"])\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"training_data_stats.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint './checkpoints/ganga-2-1b-embeddings-new-equall-eos-42-epoch-1/checkpoint-15000/global_step15000'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 1/1 [00:00<00:00, 1265.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected checkpoint of type zero stage 3, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.16.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering sharded weights: 100%|██████████| 146/146 [00:00<00:00, 741366.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Trainable fp32 state dict with 146 params 939591680 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving checkpoint shards: 100%|██████████| 1/1 [00:03<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.utils.zero_to_fp32 import convert_zero_checkpoint_to_fp32_state_dict\n",
    "\n",
    "best_model_dir = \"./checkpoints/ganga-2-1b-embeddings-new-equall-eos-42-epoch-1/checkpoint-15000\"\n",
    "output_dir = \"checkpoints/ganga-2-1b-embeddings-new-equall-eos-42-epoch-1/best_model\"\n",
    "convert_zero_checkpoint_to_fp32_state_dict(best_model_dir, output_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ganga_modeling import EmbeddingModel, BidirectionalMistralConfig, BidirectionalMistralModel\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "output_dir = \"./checkpoints/ganga-2-1b-embeddings-new-equall-eos-42-epoch-1/best_model\"\n",
    "state_dict = torch.load(f\"{output_dir}/pytorch_model.bin\")\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"LingoIITGN/Ganga-2-1B\")\n",
    "\n",
    "original_config = AutoConfig.from_pretrained(\"LingoIITGN/Ganga-2-1B\")\n",
    "bidir_config = BidirectionalMistralConfig(**original_config.to_dict())\n",
    "bidir_model = BidirectionalMistralModel(bidir_config)\n",
    "\n",
    "model2 = EmbeddingModel(bidir_model, 'mean')\n",
    "model2.load_state_dict(state_dict)\n",
    "model2.base_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "paths = [\"./Processed_data/mtop_intent.jsonl\",\n",
    "         #\"./Processed_data/samanantar_language_classification.jsonl\",\n",
    "         \"./Processed_data/xnli_new.jsonl\"]\n",
    "\n",
    "finetuning_data = []\n",
    "\n",
    "for path in paths:\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        for sample in f:\n",
    "            finetuning_data.append(json.loads(sample))\n",
    "\n",
    "random.shuffle(finetuning_data)\n",
    "\n",
    "train, test = train_test_split(finetuning_data, test_size=0.1)\n",
    "\n",
    "os.makedirs(\"./new_training_data2\", exist_ok=True)\n",
    "\n",
    "with open(\"./new_training_data2/train_data.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in train:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(\"./new_training_data2/val_data.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in test:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./new_training_data/train_data_with_hard_negative.jsonl\", \"r\") as f:\n",
    "    train_data = []\n",
    "    for sample in f:\n",
    "        train_data.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hinvec/miniconda3/envs/AI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "DataFilesNotFoundError",
     "evalue": "No (supported) data files found in Data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataFilesNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/AI/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/AI/lib/python3.10/site-packages/datasets/load.py:1849\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1848\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1849\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/AI/lib/python3.10/site-packages/datasets/load.py:1586\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LocalDatasetModuleFactoryWithScript(\n\u001b[1;32m   1578\u001b[0m         combined_path,\n\u001b[1;32m   1579\u001b[0m         download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1580\u001b[0m         dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1581\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   1582\u001b[0m     )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[0;32m-> 1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;66;03m# Try remotely\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_relative_path(path) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/AI/lib/python3.10/site-packages/datasets/load.py:842\u001b[0m, in \u001b[0;36mLocalDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m get_data_patterns(base_path)\n\u001b[1;32m    837\u001b[0m data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_patterns(\n\u001b[1;32m    838\u001b[0m     patterns,\n\u001b[1;32m    839\u001b[0m     base_path\u001b[38;5;241m=\u001b[39mbase_path,\n\u001b[1;32m    840\u001b[0m     allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    841\u001b[0m )\n\u001b[0;32m--> 842\u001b[0m module_name, default_builder_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43minfer_module_for_data_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m data_files \u001b[38;5;241m=\u001b[39m data_files\u001b[38;5;241m.\u001b[39mfilter_extensions(_MODULE_TO_EXTENSIONS[module_name])\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# Collect metadata files if the module supports them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/AI/lib/python3.10/site-packages/datasets/load.py:603\u001b[0m, in \u001b[0;36minfer_module_for_data_files\u001b[0;34m(data_files, path, download_config)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt infer the same data file format for all splits. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name:\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DataFilesNotFoundError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo (supported) data files found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module_name, default_builder_kwargs\n",
      "\u001b[0;31mDataFilesNotFoundError\u001b[0m: No (supported) data files found in Data"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"./Data\", split='train', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_parquet(\"./Data/0000.parquet?download=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>eng_Latn</th>\n",
       "      <th>asm_Beng</th>\n",
       "      <th>hin_Deva</th>\n",
       "      <th>mal_Mlym</th>\n",
       "      <th>ben_Beng</th>\n",
       "      <th>guj_Gujr</th>\n",
       "      <th>san_Deva</th>\n",
       "      <th>kan_Knda</th>\n",
       "      <th>tel_Telu</th>\n",
       "      <th>mar_Deva</th>\n",
       "      <th>tam_Taml</th>\n",
       "      <th>ory_Orya</th>\n",
       "      <th>npi_Deva</th>\n",
       "      <th>pan_Guru</th>\n",
       "      <th>urd_Arab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000052b8e57f36643caeeb7d1748e3d99a0cde0097db8...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=61293793</td>\n",
       "      <td>Bigger (Beyoncé song)</td>\n",
       "      <td>\\nBigger (Beyoncé song)\\n\\n\"Bigger\" (stylized ...</td>\n",
       "      <td>\\nডাঙৰ (বেয় 'নচে গীত)\\n\\n\"ডাঙৰ\" (ডাঙৰ বাক্যত ...</td>\n",
       "      <td>\\nबड़ा (बेयोंसे गीत)\\n\\n\"बड़ा\" (बड़े अक्षर में...</td>\n",
       "      <td>\\nവലിയ (ബിയോൺസ് ഗാനം)\\n\\n\"2019-ലെ ആൽബത്തിൽ നിന...</td>\n",
       "      <td>\\nবড় (বেয়োন্সে গান)\\n\\n\"বড়\" (বড় হাতের শৈলী...</td>\n",
       "      <td>\\nમોટું (બિયોન્સે ગીત)\\n\\n\"\" \"મોટું\" \"(મોટા અક...</td>\n",
       "      <td>\\nबृहत् (बियोन्से गीतम्)\\n\\n\"बिग्\" (बृहत्कृत्य...</td>\n",
       "      <td>\\nದೊಡ್ಡದು (ಬಿಯಾನ್ಸ್ ಹಾಡು)\\n\\n\"ದೊಡ್ಡದು\" (ದೊಡ್ಡ ...</td>\n",
       "      <td>\\nపెద్దది (బియాన్స్ పాట)\\n\\n\"పెద్దది\" (పెద్ద అ...</td>\n",
       "      <td>\\nमोठे (बियॉन्से गाणे)\\n\\n\"मोठे\" (मोठ्या अक्षर...</td>\n",
       "      <td>\\nபெரிய (பியோனஸ் பாடல்)\\n\\n\"பெரியது\" (பெரிய எழ...</td>\n",
       "      <td>\\nବଡ଼ (ବିଯ଼ୋନ୍ସେ ଗୀତ)\\n\\n\"ବଡ଼\" (ବଡ଼ ଅକ୍ଷରରେ ଶୈ...</td>\n",
       "      <td>\\nठुलो (बियोन्से गीत)\\n\\n\"ठुलो\" (ठुलो अक्षरमा ...</td>\n",
       "      <td>\\nਵੱਡਾ (ਬੇਓਨਸੇ ਗੀਤ)\\n\\n\"ਵੱਡਾ\" (ਵੱਡੇ ਅੱਖਰ ਵਿੱਚ ...</td>\n",
       "      <td>\\nبڑا (بیونسی گانا)\\n\\n\"بڑا\" (بڑے حروف میں اسٹ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000005ccabca8ce6ec2429b6b42b513ddad4dd32e61ec9...</td>\n",
       "      <td>https://en.wiktionary.org/wiki?curid=8195905</td>\n",
       "      <td>이다</td>\n",
       "      <td>\\n이다\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...</td>\n",
       "      <td>কোৰিয়ান। ব্যুৎপত্তি 2. ক্রিয়া। সংমিশ্ৰণ। টোক...</td>\n",
       "      <td>कोरियाई। व्युत्पत्ति 2. क्रिया। संयुग्मन। नोटः...</td>\n",
       "      <td>കൊറിയൻ. പദവ്യുത്പത്തി 2. ക്രിയ. സംയോജനം. കുറിപ...</td>\n",
       "      <td>কোরিয়ান। ব্যুৎপত্তি 2. ক্রিয়া। সংমিশ্রণ। দ্র...</td>\n",
       "      <td>કોરિયન. વ્યુત્પત્તિ 2. ક્રિયાપદ. સંયોજન. નોંધઃ...</td>\n",
       "      <td>कोरिया-देशः। व्युत्पत्ति 2. क्रियापद। संयोगः। ...</td>\n",
       "      <td>ಕೊರಿಯನ್. ವ್ಯುತ್ಪತ್ತಿ 2. ಕ್ರಿಯಾಪದ. ಸಂಯೋಗ. ಗಮನಿಸ...</td>\n",
       "      <td>కొరియన్. వ్యుత్పత్తి శాస్త్రం 2. క్రియ. సంయోగం...</td>\n",
       "      <td>कोरियन. व्युत्पत्तीशास्त्र 2. क्रियापद. संयोग....</td>\n",
       "      <td>கொரியன். சொற்பிறப்பியல் 2. வினைச்சொல். இணைத்தல...</td>\n",
       "      <td>କୋରିଆ। ବ୍ଯ଼ୁତ୍ପତ୍ତି 2. କ୍ରିଯ଼ା। ସଂଯୋଗ। ଟିପ୍ପଣୀ...</td>\n",
       "      <td>कोरियन। व्युत्पत्ति 2. क्रिया। संयोजन। नोटः जब...</td>\n",
       "      <td>ਕੋਰੀਆਈ. ਸ਼ਬਦ-ਵਿਗਿਆਨ 2. ਕ੍ਰਿਆ। ਸੰਯੋਜਨ. ਨੋਟਃ ਹਾਲ...</td>\n",
       "      <td>کوریائی۔ فعلیات 2. فعل۔ کنجگیشن۔ نوٹ: اگرچہ تج...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000006115d7c2d887cd80a9be89eb9ae4165d5627d9afd...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=57257423</td>\n",
       "      <td>Harriet White Fisher</td>\n",
       "      <td>\\nHarriet White Fisher\\n\\nHarriet White Fisher...</td>\n",
       "      <td>\\nহেৰিয়েট বগা মাছমৰীয়া\\n\\nহেৰিয়েট হোৱাইট ফি...</td>\n",
       "      <td>\\nहैरियट सफेद मछुआरा\\n\\nहैरियट व्हाइट फिशर एंड...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nহ্যারিয়েট সাদা মাছরাঙা\\n\\nহ্যারিয়েট হোয়াই...</td>\n",
       "      <td>\\nહેરિયટ સફેદ માછીમાર\\n\\nહેરિયટ વ્હાઇટ ફિશર એન...</td>\n",
       "      <td>\\nह्यारियेट् श्वेतमत्स्यपालकः\\n\\nह्यारियेट् वै...</td>\n",
       "      <td>\\nಹ್ಯಾರಿಯೆಟ್ ಬಿಳಿ ಮೀನುಗಾರ\\n\\nಹ್ಯಾರಿಯೆಟ್ ವೈಟ್ ಫ...</td>\n",
       "      <td>\\nహారియట్ వైట్ ఫిషర్\\n\\nహారియెట్ వైట్ ఫిషర్ ఆం...</td>\n",
       "      <td>\\nहॅरियट पांढरा मच्छीमार\\n\\nहॅरियट व्हाईट फिशर...</td>\n",
       "      <td>\\nஹாரியட் வெள்ளை மீனவர்\\n\\nஹாரியட் வெள்ளை மீனவ...</td>\n",
       "      <td>\\nହେରିଏଟ୍ ଧଳା ମତ୍ସ୍ଯ଼ଜୀବୀ\\n\\nହେରିଏଟ୍ ହ୍ୱାଇଟ୍ ଫ...</td>\n",
       "      <td>\\nह्यारियट सेतो माछा मार्ने\\n\\nह्यारिएट ह्वाइट...</td>\n",
       "      <td>\\nਹੈਰੀਏਟ ਚਿੱਟਾ ਮਛੇਰਾ\\n\\nਹੈਰੀਏਟ ਵ੍ਹਾਈਟ ਫਿਸ਼ਰ ਐਂ...</td>\n",
       "      <td>\\nہیریٹ وائٹ فشر\\n\\nہیریٹ وائٹ فشر اینڈریو (18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000736d1c67c08e28234ea2e1bb4b3157c6db7d52bb6...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=86020</td>\n",
       "      <td>Iris (mythology)</td>\n",
       "      <td>\\nIris (mythology)\\n\\nAncient Greek personific...</td>\n",
       "      <td>\\nআইৰিছ (পৌৰাণিক কাহিনী)\\n\\nপ্ৰাচীন গ্ৰীক ধৰ্ম...</td>\n",
       "      <td>\\nआइरिस (पौराणिक कथा)\\n\\nप्राचीन यूनानी धर्म औ...</td>\n",
       "      <td>\\nഐറിസ് (പുരാണം)\\n\\nപുരാതന ഗ്രീക്ക് മതത്തിലും ...</td>\n",
       "      <td>\\nআইরিস (পৌরাণিক কাহিনী)\\n\\nপ্রাচীন গ্রীক ধর্ম...</td>\n",
       "      <td>\\nઆઇરિસ (પૌરાણિક કથા)\\n\\nપ્રાચીન ગ્રીક ધર્મ અન...</td>\n",
       "      <td>\\nऐरीस् (पौराणिक कथा)\\n\\nप्राचीन-ग्रीक्-धर्मस्...</td>\n",
       "      <td>\\nಐರಿಸ್ (ಪುರಾಣ)\\n\\nಪ್ರಾಚೀನ ಗ್ರೀಕ್ ಧರ್ಮ ಮತ್ತು ಪ...</td>\n",
       "      <td>\\nఐరిస్ (పురాణం)\\n\\nపురాతన గ్రీకు మతం మరియు పు...</td>\n",
       "      <td>\\nआयरिस (पौराणिक कथा)\\n\\nप्राचीन ग्रीक धर्म आण...</td>\n",
       "      <td>\\nஐரிஸ் (புராணம்)\\n\\nபண்டைய கிரேக்க மதம் மற்று...</td>\n",
       "      <td>\\nଆଇରିସ (ପୌରାଣିକ କଥା)\\n\\nପ୍ରାଚୀନ ଗ୍ରୀକ୍ ଧର୍ମ ଏ...</td>\n",
       "      <td>\\nआइरिस (पौराणिक कथा)\\n\\nप्राचीन ग्रिक धर्म र ...</td>\n",
       "      <td>\\nਆਈਰਿਸ (ਮਿਥਿਹਾਸ)\\n\\nਪ੍ਰਾਚੀਨ ਯੂਨਾਨੀ ਧਰਮ ਅਤੇ ਮਿ...</td>\n",
       "      <td>\\nآئیرس (افسانے)\\n\\nقدیم یونانی مذہب اور افسان...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000768be7a3eb0f4fa50bfaf2a10f864e44c24c48342...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=64450824</td>\n",
       "      <td>Ioannis Kontoyiannis</td>\n",
       "      <td>\\nIoannis Kontoyiannis\\n\\nGreek mathematician ...</td>\n",
       "      <td>\\nইয়োনিছ কনটোয়িয়ানিছ\\n\\nইঅ 'নিছ কন্টয়ানিছ ...</td>\n",
       "      <td>\\nइओनिस कोंटोयनिस\\n\\nइओनिस कोंटोयनिस (जन्म जनव...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nইয়নিস কনটোয়িয়ান্নিস\\n\\nইওনিস কনটোয়িয়ান্...</td>\n",
       "      <td>\\nઇઓનિસ કોન્ટોયિયાનીસ\\n\\nઇઓનિસ કોન્ટોયિયાનીસ (...</td>\n",
       "      <td>\\nइयोनिस् कोण्टोयानिस्\\n\\nइयोनिस् कोण्टोयानिस्...</td>\n",
       "      <td>\\nಇಯೋನಿಸ್ ಕೊಂಟೊಯನ್ನಿಸ್\\n\\nಇಯೋನಿಸ್ ಕೊಂಟೊಯನ್ನಿಸ್...</td>\n",
       "      <td>\\nఇయోనిస్ కొంటోయన్నిస్\\n\\nఇయోనిస్ కొంటోయాన్నిస...</td>\n",
       "      <td>\\nइओनिस कोंटोयॅनिस\\n\\nइओनिस कोंटोयॅनिस (जन्म ज...</td>\n",
       "      <td>\\nஅயோனிஸ் கொன்டோயியன்னிஸ்\\n\\nஅயோனிஸ் கொன்டோயான...</td>\n",
       "      <td>\\nଆଯ଼ୋନିଜ୍ କୋଣ୍ଟୋଯ଼ିଆନିସ୍\\n\\nଆଇଓନିଜ୍ କୋଣ୍ଟୋଯ଼ି...</td>\n",
       "      <td>\\nइओनिस कन्टोयनिस\\n\\nइओनिस कोन्टोयनिस (जन्म जन...</td>\n",
       "      <td>\\nਆਇਓਨਿਸ ਕੋਂਟੋਇਆਨਿਸ\\n\\nਇਓਨਿਸ ਕੋਂਟੋਇਆਨਿਸ (ਜਨਮ ਜ...</td>\n",
       "      <td>\\nآئیونیس کونٹوئینس\\n\\nآئیونیس کونٹوئینس (پیدا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22266</th>\n",
       "      <td>01030f0c76ee812c9a71ddc6c0a7cd5365f154bc71d5fc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=29701458</td>\n",
       "      <td>List of Oceanian records in track cycling</td>\n",
       "      <td>\\nList of Oceanian records in track cycling\\n\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nट्रैक साइकिलिंग में महासागरीय रिकॉर्ड की सूच...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nট্র্যাক সাইক্লিং-এ মহাসাগরীয় রেকর্ডের তালিক...</td>\n",
       "      <td>\\nટ્રેક સાઇકલિંગમાં મહાસાગરીય રેકોર્ડની યાદી\\n...</td>\n",
       "      <td>\\nट्र्याक्-सैक्लिङ्ग् मध्ये ओशिनियन्-अभिलेखाना...</td>\n",
       "      <td>\\nಟ್ರ್ಯಾಕ್ ಸೈಕ್ಲಿಂಗ್ನಲ್ಲಿ ಸಾಗರ ದಾಖಲೆಗಳ ಪಟ್ಟಿ\\n...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nट्रॅक सायकलिंगमधील सागरी विक्रमांची यादी\\n\\n...</td>\n",
       "      <td>\\nதடம் சைக்கிள் ஓட்டுவதில் கடல் பதிவுகளின் பட்...</td>\n",
       "      <td>\\nଟ୍ରାକ୍ ସାଇକ୍ଲିଂରେ ମହାସାଗରୀଯ଼ ରେକର୍ଡର ତାଲିକା\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nਟਰੈਕ ਸਾਈਕਲਿੰਗ ਵਿੱਚ ਸਮੁੰਦਰੀ ਰਿਕਾਰਡ ਦੀ ਸੂਚੀ\\n\\...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22267</th>\n",
       "      <td>010310f9d9135c2ac0c01cd48dc1fedc3c8ba27910d5de...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=177952</td>\n",
       "      <td>Great Zimbabwe</td>\n",
       "      <td>\\nGreat Zimbabwe\\n\\nRuins of a medieval city i...</td>\n",
       "      <td>\\nগ্ৰেট জিম্বাবৱে\\n\\nগ্ৰেট জিম্বাবৱে হৈছে আধুন...</td>\n",
       "      <td>\\nमहान जिम्बाब्वे\\n\\nमहान जिम्बाब्वे आधुनिक जि...</td>\n",
       "      <td>\\nഗ്രേറ്റ് സിംബാബ്വെ\\n\\nആധുനിക രാജ്യമായ സിംബാബ...</td>\n",
       "      <td>\\nগ্রেট জিম্বাবুয়ে\\n\\nগ্রেট জিম্বাবুয়ে হল আধ...</td>\n",
       "      <td>\\nગ્રેટ ઝિમ્બાબ્વે\\n\\nગ્રેટ ઝિમ્બાબ્વે એ આધુનિ...</td>\n",
       "      <td>\\nग्रेट् जिम्बाब्वे\\n\\nग्रेट् जिम्बाब्वे इति आ...</td>\n",
       "      <td>\\nಗ್ರೇಟ್ ಜಿಂಬಾಬ್ವೆ\\n\\nಗ್ರೇಟ್ ಜಿಂಬಾಬ್ವೆ ಆಧುನಿಕ ...</td>\n",
       "      <td>\\nగ్రేట్ జింబాబ్వే\\n\\nగ్రేట్ జింబాబ్వే అనేది ఆ...</td>\n",
       "      <td>\\nमहान झिम्बाब्वे\\n\\nग्रेट झिम्बाब्वे हे आधुनि...</td>\n",
       "      <td>\\nபெரிய ஜிம்பாப்வே\\n\\nகிரேட் ஜிம்பாப்வே என்பது...</td>\n",
       "      <td>\\nଗ୍ରେଟ ଜିମ୍ବାୱେ\\n\\nଗ୍ରେଟ ଜିମ୍ବାୱେ ହେଉଛି ଆଧୁନି...</td>\n",
       "      <td>\\nग्रेट जिम्बाब्वे\\n\\nग्रेट जिम्बाब्वे आधुनिक ...</td>\n",
       "      <td>\\nਮਹਾਨ ਜ਼ਿੰਬਾਬਵੇ\\n\\nਮਹਾਨ ਜ਼ਿੰਬਾਬਵੇ ਆਧੁਨਿਕ ਦੇਸ਼...</td>\n",
       "      <td>\\nعظیم زمبابوے\\n\\nعظیم زمبابوے جدید ملک زمبابو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22268</th>\n",
       "      <td>01031122c84ee82f58e0b3675cfb683d18e2eb3ace4c1f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=24759609</td>\n",
       "      <td>Alberto Muñoz</td>\n",
       "      <td>\\nAlberto Muñoz\\n\\nMexican professional wrestl...</td>\n",
       "      <td>\\nআলবাৰ্টো মুনোজ\\n\\nইছমেল মুনোজ লোপেজ (ইংৰাজীঃ...</td>\n",
       "      <td>\\nअल्बर्टो मुनोज़\\n\\nइस्माइल मुनोज़ लोपेज़ (15...</td>\n",
       "      <td>\\nആൽബെർട്ടോ മുനോസ്\\n\\nആൽബെർട്ടോ മുനോസ് എന്ന റി...</td>\n",
       "      <td>\\nআলবার্তো মুনোজ\\n\\nইসমাইল মুনোজ লোপেজ (15ই জা...</td>\n",
       "      <td>\\nઆલ્બર્ટો મુનોઝ\\n\\nઇસમાઇલ મુનોઝ લોપેઝ (15 જાન...</td>\n",
       "      <td>\\nअल्बर्टो मुनोज़्\\n\\nइस्मायिल् मुनोज़् लोपेज़...</td>\n",
       "      <td>\\nಅಲ್ಬರ್ಟೊ ಮುನೋಜ್\\n\\nಇಸ್ಮಾಯೆಲ್ ಮುನೋಜ್ ಲೋಪೆಜ್ (...</td>\n",
       "      <td>\\nఅల్బెర్టో మునోజ్\\n\\nఇస్మాయిల్ మునోజ్ లోపెజ్ ...</td>\n",
       "      <td>\\nअल्बर्टो मुनोझ\\n\\nइस्मायेल मुनोझ लोपेझ (15 ज...</td>\n",
       "      <td>\\nஅல்பெர்டோ முனோஸ்\\n\\nஇஸ்மாயில் முனோஸ் லோபஸ் (...</td>\n",
       "      <td>\\nଆଲବର୍ଟୋ ମୁନୋଜ୍\\n\\nଇସମାଇଲ ମୁନୋଜ ଲୋପେଜ (ଜାନୁଆର...</td>\n",
       "      <td>\\nअल्बर्टो मुनोज\\n\\nइस्मेल मुनोज लोपेज (जनवरी ...</td>\n",
       "      <td>\\nਅਲਬਰਟੋ ਮੁਨੋਜ਼\\n\\nਇਸਮਾਇਲ ਮੁਨੋਜ਼ ਲੋਪੇਜ਼ (15 ਜਨ...</td>\n",
       "      <td>\\nالبرٹو مونوز\\n\\nاسماعیل منوز لوپیز (15 جنوری...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22269</th>\n",
       "      <td>01031156f9e8456585fa6b657244f50df3bdf240e9c31b...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=17039720</td>\n",
       "      <td>A Vedic Word Concordance</td>\n",
       "      <td>\\nA Vedic Word Concordance\\n\\nMulti-volume con...</td>\n",
       "      <td>\\nএটা বৈদিক শব্দৰ সমন্বয়\\n\\nবৈদিক শব্দ কনকোৰ্...</td>\n",
       "      <td>\\nएक वैदिक शब्द समन्वय\\n\\nएक वैदिक शब्द समन्वय...</td>\n",
       "      <td>\\nഒരു വേദ വാക്ക് കോൺകോർഡൻസ്\\n\\nവൈദിക സംസ്കൃത ഗ...</td>\n",
       "      <td>\\nএকটি বৈদিক শব্দ সঙ্গতি\\n\\nএকটি বৈদিক শব্দ সঙ...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\nवैदिकशब्दः समन्वयः\\n\\nवैदिकशब्दः समन्वयः (सं...</td>\n",
       "      <td>\\nವೈದಿಕ ಪದದ ಸಾಮರಸ್ಯ\\n\\nವೈದಿಕ ಪದವಾದ ಸಮನ್ವಯವು (ಸ...</td>\n",
       "      <td>\\nవేద పదం సామరస్యం\\n\\nవేద పదం సామరస్యం (సంస్కృ...</td>\n",
       "      <td>\\nएक वैदिक शब्द समन्वय\\n\\nवैदिक शब्द समन्वय (स...</td>\n",
       "      <td>\\nஒரு வேத வார்த்தை இணக்கம்\\n\\nவேதச் சொல் ஒருங்...</td>\n",
       "      <td>\\nଏକ ବୈଦିକ ଶବ୍ଦ ସମନ୍ୱଯ଼\\n\\nଏକ ବୈଦିକ ଶବ୍ଦ 'କନକୋ...</td>\n",
       "      <td>\\nवैदिक शब्द समन्वय\\n\\nवैदिक शब्द समन्वय (संस्...</td>\n",
       "      <td>\\nਇੱਕ ਵੈਦਿਕ ਸ਼ਬਦ ਸੰਜੋਗ\\n\\nਵੈਦਿਕ ਸ਼ਬਦ ਸੰਜੋਗ (ਸੰ...</td>\n",
       "      <td>\\nایک ویدک لفظ مطابقت\\n\\nویدک لفظ مطابقت (سنسک...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22270</th>\n",
       "      <td>010312b5c43847bef34bf7a718d9270c3f8cec5a34014a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=43029586</td>\n",
       "      <td>Jason Alkire</td>\n",
       "      <td>\\nJason Alkire\\n\\nAmerican fashion designer\\nJ...</td>\n",
       "      <td>\\nজেছন এলকিৰে\\n\\nজেছন এলকিৰে এগৰাকী আমেৰিকান ফ...</td>\n",
       "      <td>\\nजेसन अल्किरे\\n\\nजेसन अल्किरे एक अमेरिकी फैशन...</td>\n",
       "      <td>\\nജേസൺ ആൽക്കൈർ\\n\\nഒരു അമേരിക്കൻ ഫാഷൻ ഡിസൈനറും ...</td>\n",
       "      <td>\\nজেসন অ্যালকির\\n\\nজেসন অ্যালকির একজন মার্কিন ...</td>\n",
       "      <td>\\nજેસન અલ્કિરે\\n\\nજેસન આલ્કિર એક અમેરિકન ફેશન ...</td>\n",
       "      <td>\\nजेसन् अल्किरे\\n\\nजासन् अल्कैर् इत्ययं अमेरिक...</td>\n",
       "      <td>\\nಜಾಸನ್ ಅಲ್ಕೈರ್\\n\\nಜಾಸನ್ ಅಲ್ಕೈರ್ ಒಬ್ಬ ಅಮೇರಿಕನ್...</td>\n",
       "      <td>\\nజాసన్ అల్కైర్\\n\\nజాసన్ ఆల్కైర్ ఒక అమెరికన్ ఫ...</td>\n",
       "      <td>\\nजेसन अल्किरे\\n\\nजॅसन अल्किरे हे एक अमेरिकन फ...</td>\n",
       "      <td>\\nஜேசன் அல்கைர்\\n\\nஜேசன் அல்கைர் ஒரு அமெரிக்க ...</td>\n",
       "      <td>\\nଜାସନ୍ ଆଲ୍କିରେ\\n\\nଜାସନ୍ ଆଲ୍କିରେ ଜଣେ ଆମେରିକୀଯ଼...</td>\n",
       "      <td>\\nजेसन अल्किरे\\n\\nजेसन अल्किरे एक अमेरिकी फेसन...</td>\n",
       "      <td>\\nਜੈਸਨ ਅਲਕੀਰੇ\\n\\nਜੈਸਨ ਅਲਕੀਰੇ ਇੱਕ ਅਮਰੀਕੀ ਫੈਸ਼ਨ ...</td>\n",
       "      <td>\\nجیسن الکائر\\n\\nجیسن الکائر ایک امریکی فیشن ڈ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22271 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  doc_id  \\\n",
       "0      0000052b8e57f36643caeeb7d1748e3d99a0cde0097db8...   \n",
       "1      000005ccabca8ce6ec2429b6b42b513ddad4dd32e61ec9...   \n",
       "2      000006115d7c2d887cd80a9be89eb9ae4165d5627d9afd...   \n",
       "3      00000736d1c67c08e28234ea2e1bb4b3157c6db7d52bb6...   \n",
       "4      00000768be7a3eb0f4fa50bfaf2a10f864e44c24c48342...   \n",
       "...                                                  ...   \n",
       "22266  01030f0c76ee812c9a71ddc6c0a7cd5365f154bc71d5fc...   \n",
       "22267  010310f9d9135c2ac0c01cd48dc1fedc3c8ba27910d5de...   \n",
       "22268  01031122c84ee82f58e0b3675cfb683d18e2eb3ace4c1f...   \n",
       "22269  01031156f9e8456585fa6b657244f50df3bdf240e9c31b...   \n",
       "22270  010312b5c43847bef34bf7a718d9270c3f8cec5a34014a...   \n",
       "\n",
       "                                                url  \\\n",
       "0      https://en.wikipedia.org/wiki?curid=61293793   \n",
       "1      https://en.wiktionary.org/wiki?curid=8195905   \n",
       "2      https://en.wikipedia.org/wiki?curid=57257423   \n",
       "3         https://en.wikipedia.org/wiki?curid=86020   \n",
       "4      https://en.wikipedia.org/wiki?curid=64450824   \n",
       "...                                             ...   \n",
       "22266  https://en.wikipedia.org/wiki?curid=29701458   \n",
       "22267    https://en.wikipedia.org/wiki?curid=177952   \n",
       "22268  https://en.wikipedia.org/wiki?curid=24759609   \n",
       "22269  https://en.wikipedia.org/wiki?curid=17039720   \n",
       "22270  https://en.wikipedia.org/wiki?curid=43029586   \n",
       "\n",
       "                                           title  \\\n",
       "0                          Bigger (Beyoncé song)   \n",
       "1                                             이다   \n",
       "2                           Harriet White Fisher   \n",
       "3                               Iris (mythology)   \n",
       "4                           Ioannis Kontoyiannis   \n",
       "...                                          ...   \n",
       "22266  List of Oceanian records in track cycling   \n",
       "22267                             Great Zimbabwe   \n",
       "22268                              Alberto Muñoz   \n",
       "22269                   A Vedic Word Concordance   \n",
       "22270                               Jason Alkire   \n",
       "\n",
       "                                                eng_Latn  \\\n",
       "0      \\nBigger (Beyoncé song)\\n\\n\"Bigger\" (stylized ...   \n",
       "1      \\n이다\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...   \n",
       "2      \\nHarriet White Fisher\\n\\nHarriet White Fisher...   \n",
       "3      \\nIris (mythology)\\n\\nAncient Greek personific...   \n",
       "4      \\nIoannis Kontoyiannis\\n\\nGreek mathematician ...   \n",
       "...                                                  ...   \n",
       "22266  \\nList of Oceanian records in track cycling\\n\\...   \n",
       "22267  \\nGreat Zimbabwe\\n\\nRuins of a medieval city i...   \n",
       "22268  \\nAlberto Muñoz\\n\\nMexican professional wrestl...   \n",
       "22269  \\nA Vedic Word Concordance\\n\\nMulti-volume con...   \n",
       "22270  \\nJason Alkire\\n\\nAmerican fashion designer\\nJ...   \n",
       "\n",
       "                                                asm_Beng  \\\n",
       "0      \\nডাঙৰ (বেয় 'নচে গীত)\\n\\n\"ডাঙৰ\" (ডাঙৰ বাক্যত ...   \n",
       "1      কোৰিয়ান। ব্যুৎপত্তি 2. ক্রিয়া। সংমিশ্ৰণ। টোক...   \n",
       "2      \\nহেৰিয়েট বগা মাছমৰীয়া\\n\\nহেৰিয়েট হোৱাইট ফি...   \n",
       "3      \\nআইৰিছ (পৌৰাণিক কাহিনী)\\n\\nপ্ৰাচীন গ্ৰীক ধৰ্ম...   \n",
       "4      \\nইয়োনিছ কনটোয়িয়ানিছ\\n\\nইঅ 'নিছ কন্টয়ানিছ ...   \n",
       "...                                                  ...   \n",
       "22266                                               None   \n",
       "22267  \\nগ্ৰেট জিম্বাবৱে\\n\\nগ্ৰেট জিম্বাবৱে হৈছে আধুন...   \n",
       "22268  \\nআলবাৰ্টো মুনোজ\\n\\nইছমেল মুনোজ লোপেজ (ইংৰাজীঃ...   \n",
       "22269  \\nএটা বৈদিক শব্দৰ সমন্বয়\\n\\nবৈদিক শব্দ কনকোৰ্...   \n",
       "22270  \\nজেছন এলকিৰে\\n\\nজেছন এলকিৰে এগৰাকী আমেৰিকান ফ...   \n",
       "\n",
       "                                                hin_Deva  \\\n",
       "0      \\nबड़ा (बेयोंसे गीत)\\n\\n\"बड़ा\" (बड़े अक्षर में...   \n",
       "1      कोरियाई। व्युत्पत्ति 2. क्रिया। संयुग्मन। नोटः...   \n",
       "2      \\nहैरियट सफेद मछुआरा\\n\\nहैरियट व्हाइट फिशर एंड...   \n",
       "3      \\nआइरिस (पौराणिक कथा)\\n\\nप्राचीन यूनानी धर्म औ...   \n",
       "4      \\nइओनिस कोंटोयनिस\\n\\nइओनिस कोंटोयनिस (जन्म जनव...   \n",
       "...                                                  ...   \n",
       "22266  \\nट्रैक साइकिलिंग में महासागरीय रिकॉर्ड की सूच...   \n",
       "22267  \\nमहान जिम्बाब्वे\\n\\nमहान जिम्बाब्वे आधुनिक जि...   \n",
       "22268  \\nअल्बर्टो मुनोज़\\n\\nइस्माइल मुनोज़ लोपेज़ (15...   \n",
       "22269  \\nएक वैदिक शब्द समन्वय\\n\\nएक वैदिक शब्द समन्वय...   \n",
       "22270  \\nजेसन अल्किरे\\n\\nजेसन अल्किरे एक अमेरिकी फैशन...   \n",
       "\n",
       "                                                mal_Mlym  \\\n",
       "0      \\nവലിയ (ബിയോൺസ് ഗാനം)\\n\\n\"2019-ലെ ആൽബത്തിൽ നിന...   \n",
       "1      കൊറിയൻ. പദവ്യുത്പത്തി 2. ക്രിയ. സംയോജനം. കുറിപ...   \n",
       "2                                                   None   \n",
       "3      \\nഐറിസ് (പുരാണം)\\n\\nപുരാതന ഗ്രീക്ക് മതത്തിലും ...   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "22266                                               None   \n",
       "22267  \\nഗ്രേറ്റ് സിംബാബ്വെ\\n\\nആധുനിക രാജ്യമായ സിംബാബ...   \n",
       "22268  \\nആൽബെർട്ടോ മുനോസ്\\n\\nആൽബെർട്ടോ മുനോസ് എന്ന റി...   \n",
       "22269  \\nഒരു വേദ വാക്ക് കോൺകോർഡൻസ്\\n\\nവൈദിക സംസ്കൃത ഗ...   \n",
       "22270  \\nജേസൺ ആൽക്കൈർ\\n\\nഒരു അമേരിക്കൻ ഫാഷൻ ഡിസൈനറും ...   \n",
       "\n",
       "                                                ben_Beng  \\\n",
       "0      \\nবড় (বেয়োন্সে গান)\\n\\n\"বড়\" (বড় হাতের শৈলী...   \n",
       "1      কোরিয়ান। ব্যুৎপত্তি 2. ক্রিয়া। সংমিশ্রণ। দ্র...   \n",
       "2      \\nহ্যারিয়েট সাদা মাছরাঙা\\n\\nহ্যারিয়েট হোয়াই...   \n",
       "3      \\nআইরিস (পৌরাণিক কাহিনী)\\n\\nপ্রাচীন গ্রীক ধর্ম...   \n",
       "4      \\nইয়নিস কনটোয়িয়ান্নিস\\n\\nইওনিস কনটোয়িয়ান্...   \n",
       "...                                                  ...   \n",
       "22266  \\nট্র্যাক সাইক্লিং-এ মহাসাগরীয় রেকর্ডের তালিক...   \n",
       "22267  \\nগ্রেট জিম্বাবুয়ে\\n\\nগ্রেট জিম্বাবুয়ে হল আধ...   \n",
       "22268  \\nআলবার্তো মুনোজ\\n\\nইসমাইল মুনোজ লোপেজ (15ই জা...   \n",
       "22269  \\nএকটি বৈদিক শব্দ সঙ্গতি\\n\\nএকটি বৈদিক শব্দ সঙ...   \n",
       "22270  \\nজেসন অ্যালকির\\n\\nজেসন অ্যালকির একজন মার্কিন ...   \n",
       "\n",
       "                                                guj_Gujr  \\\n",
       "0      \\nમોટું (બિયોન્સે ગીત)\\n\\n\"\" \"મોટું\" \"(મોટા અક...   \n",
       "1      કોરિયન. વ્યુત્પત્તિ 2. ક્રિયાપદ. સંયોજન. નોંધઃ...   \n",
       "2      \\nહેરિયટ સફેદ માછીમાર\\n\\nહેરિયટ વ્હાઇટ ફિશર એન...   \n",
       "3      \\nઆઇરિસ (પૌરાણિક કથા)\\n\\nપ્રાચીન ગ્રીક ધર્મ અન...   \n",
       "4      \\nઇઓનિસ કોન્ટોયિયાનીસ\\n\\nઇઓનિસ કોન્ટોયિયાનીસ (...   \n",
       "...                                                  ...   \n",
       "22266  \\nટ્રેક સાઇકલિંગમાં મહાસાગરીય રેકોર્ડની યાદી\\n...   \n",
       "22267  \\nગ્રેટ ઝિમ્બાબ્વે\\n\\nગ્રેટ ઝિમ્બાબ્વે એ આધુનિ...   \n",
       "22268  \\nઆલ્બર્ટો મુનોઝ\\n\\nઇસમાઇલ મુનોઝ લોપેઝ (15 જાન...   \n",
       "22269                                               None   \n",
       "22270  \\nજેસન અલ્કિરે\\n\\nજેસન આલ્કિર એક અમેરિકન ફેશન ...   \n",
       "\n",
       "                                                san_Deva  \\\n",
       "0      \\nबृहत् (बियोन्से गीतम्)\\n\\n\"बिग्\" (बृहत्कृत्य...   \n",
       "1      कोरिया-देशः। व्युत्पत्ति 2. क्रियापद। संयोगः। ...   \n",
       "2      \\nह्यारियेट् श्वेतमत्स्यपालकः\\n\\nह्यारियेट् वै...   \n",
       "3      \\nऐरीस् (पौराणिक कथा)\\n\\nप्राचीन-ग्रीक्-धर्मस्...   \n",
       "4      \\nइयोनिस् कोण्टोयानिस्\\n\\nइयोनिस् कोण्टोयानिस्...   \n",
       "...                                                  ...   \n",
       "22266  \\nट्र्याक्-सैक्लिङ्ग् मध्ये ओशिनियन्-अभिलेखाना...   \n",
       "22267  \\nग्रेट् जिम्बाब्वे\\n\\nग्रेट् जिम्बाब्वे इति आ...   \n",
       "22268  \\nअल्बर्टो मुनोज़्\\n\\nइस्मायिल् मुनोज़् लोपेज़...   \n",
       "22269  \\nवैदिकशब्दः समन्वयः\\n\\nवैदिकशब्दः समन्वयः (सं...   \n",
       "22270  \\nजेसन् अल्किरे\\n\\nजासन् अल्कैर् इत्ययं अमेरिक...   \n",
       "\n",
       "                                                kan_Knda  \\\n",
       "0      \\nದೊಡ್ಡದು (ಬಿಯಾನ್ಸ್ ಹಾಡು)\\n\\n\"ದೊಡ್ಡದು\" (ದೊಡ್ಡ ...   \n",
       "1      ಕೊರಿಯನ್. ವ್ಯುತ್ಪತ್ತಿ 2. ಕ್ರಿಯಾಪದ. ಸಂಯೋಗ. ಗಮನಿಸ...   \n",
       "2      \\nಹ್ಯಾರಿಯೆಟ್ ಬಿಳಿ ಮೀನುಗಾರ\\n\\nಹ್ಯಾರಿಯೆಟ್ ವೈಟ್ ಫ...   \n",
       "3      \\nಐರಿಸ್ (ಪುರಾಣ)\\n\\nಪ್ರಾಚೀನ ಗ್ರೀಕ್ ಧರ್ಮ ಮತ್ತು ಪ...   \n",
       "4      \\nಇಯೋನಿಸ್ ಕೊಂಟೊಯನ್ನಿಸ್\\n\\nಇಯೋನಿಸ್ ಕೊಂಟೊಯನ್ನಿಸ್...   \n",
       "...                                                  ...   \n",
       "22266  \\nಟ್ರ್ಯಾಕ್ ಸೈಕ್ಲಿಂಗ್ನಲ್ಲಿ ಸಾಗರ ದಾಖಲೆಗಳ ಪಟ್ಟಿ\\n...   \n",
       "22267  \\nಗ್ರೇಟ್ ಜಿಂಬಾಬ್ವೆ\\n\\nಗ್ರೇಟ್ ಜಿಂಬಾಬ್ವೆ ಆಧುನಿಕ ...   \n",
       "22268  \\nಅಲ್ಬರ್ಟೊ ಮುನೋಜ್\\n\\nಇಸ್ಮಾಯೆಲ್ ಮುನೋಜ್ ಲೋಪೆಜ್ (...   \n",
       "22269  \\nವೈದಿಕ ಪದದ ಸಾಮರಸ್ಯ\\n\\nವೈದಿಕ ಪದವಾದ ಸಮನ್ವಯವು (ಸ...   \n",
       "22270  \\nಜಾಸನ್ ಅಲ್ಕೈರ್\\n\\nಜಾಸನ್ ಅಲ್ಕೈರ್ ಒಬ್ಬ ಅಮೇರಿಕನ್...   \n",
       "\n",
       "                                                tel_Telu  \\\n",
       "0      \\nపెద్దది (బియాన్స్ పాట)\\n\\n\"పెద్దది\" (పెద్ద అ...   \n",
       "1      కొరియన్. వ్యుత్పత్తి శాస్త్రం 2. క్రియ. సంయోగం...   \n",
       "2      \\nహారియట్ వైట్ ఫిషర్\\n\\nహారియెట్ వైట్ ఫిషర్ ఆం...   \n",
       "3      \\nఐరిస్ (పురాణం)\\n\\nపురాతన గ్రీకు మతం మరియు పు...   \n",
       "4      \\nఇయోనిస్ కొంటోయన్నిస్\\n\\nఇయోనిస్ కొంటోయాన్నిస...   \n",
       "...                                                  ...   \n",
       "22266                                               None   \n",
       "22267  \\nగ్రేట్ జింబాబ్వే\\n\\nగ్రేట్ జింబాబ్వే అనేది ఆ...   \n",
       "22268  \\nఅల్బెర్టో మునోజ్\\n\\nఇస్మాయిల్ మునోజ్ లోపెజ్ ...   \n",
       "22269  \\nవేద పదం సామరస్యం\\n\\nవేద పదం సామరస్యం (సంస్కృ...   \n",
       "22270  \\nజాసన్ అల్కైర్\\n\\nజాసన్ ఆల్కైర్ ఒక అమెరికన్ ఫ...   \n",
       "\n",
       "                                                mar_Deva  \\\n",
       "0      \\nमोठे (बियॉन्से गाणे)\\n\\n\"मोठे\" (मोठ्या अक्षर...   \n",
       "1      कोरियन. व्युत्पत्तीशास्त्र 2. क्रियापद. संयोग....   \n",
       "2      \\nहॅरियट पांढरा मच्छीमार\\n\\nहॅरियट व्हाईट फिशर...   \n",
       "3      \\nआयरिस (पौराणिक कथा)\\n\\nप्राचीन ग्रीक धर्म आण...   \n",
       "4      \\nइओनिस कोंटोयॅनिस\\n\\nइओनिस कोंटोयॅनिस (जन्म ज...   \n",
       "...                                                  ...   \n",
       "22266  \\nट्रॅक सायकलिंगमधील सागरी विक्रमांची यादी\\n\\n...   \n",
       "22267  \\nमहान झिम्बाब्वे\\n\\nग्रेट झिम्बाब्वे हे आधुनि...   \n",
       "22268  \\nअल्बर्टो मुनोझ\\n\\nइस्मायेल मुनोझ लोपेझ (15 ज...   \n",
       "22269  \\nएक वैदिक शब्द समन्वय\\n\\nवैदिक शब्द समन्वय (स...   \n",
       "22270  \\nजेसन अल्किरे\\n\\nजॅसन अल्किरे हे एक अमेरिकन फ...   \n",
       "\n",
       "                                                tam_Taml  \\\n",
       "0      \\nபெரிய (பியோனஸ் பாடல்)\\n\\n\"பெரியது\" (பெரிய எழ...   \n",
       "1      கொரியன். சொற்பிறப்பியல் 2. வினைச்சொல். இணைத்தல...   \n",
       "2      \\nஹாரியட் வெள்ளை மீனவர்\\n\\nஹாரியட் வெள்ளை மீனவ...   \n",
       "3      \\nஐரிஸ் (புராணம்)\\n\\nபண்டைய கிரேக்க மதம் மற்று...   \n",
       "4      \\nஅயோனிஸ் கொன்டோயியன்னிஸ்\\n\\nஅயோனிஸ் கொன்டோயான...   \n",
       "...                                                  ...   \n",
       "22266  \\nதடம் சைக்கிள் ஓட்டுவதில் கடல் பதிவுகளின் பட்...   \n",
       "22267  \\nபெரிய ஜிம்பாப்வே\\n\\nகிரேட் ஜிம்பாப்வே என்பது...   \n",
       "22268  \\nஅல்பெர்டோ முனோஸ்\\n\\nஇஸ்மாயில் முனோஸ் லோபஸ் (...   \n",
       "22269  \\nஒரு வேத வார்த்தை இணக்கம்\\n\\nவேதச் சொல் ஒருங்...   \n",
       "22270  \\nஜேசன் அல்கைர்\\n\\nஜேசன் அல்கைர் ஒரு அமெரிக்க ...   \n",
       "\n",
       "                                                ory_Orya  \\\n",
       "0      \\nବଡ଼ (ବିଯ଼ୋନ୍ସେ ଗୀତ)\\n\\n\"ବଡ଼\" (ବଡ଼ ଅକ୍ଷରରେ ଶୈ...   \n",
       "1      କୋରିଆ। ବ୍ଯ଼ୁତ୍ପତ୍ତି 2. କ୍ରିଯ଼ା। ସଂଯୋଗ। ଟିପ୍ପଣୀ...   \n",
       "2      \\nହେରିଏଟ୍ ଧଳା ମତ୍ସ୍ଯ଼ଜୀବୀ\\n\\nହେରିଏଟ୍ ହ୍ୱାଇଟ୍ ଫ...   \n",
       "3      \\nଆଇରିସ (ପୌରାଣିକ କଥା)\\n\\nପ୍ରାଚୀନ ଗ୍ରୀକ୍ ଧର୍ମ ଏ...   \n",
       "4      \\nଆଯ଼ୋନିଜ୍ କୋଣ୍ଟୋଯ଼ିଆନିସ୍\\n\\nଆଇଓନିଜ୍ କୋଣ୍ଟୋଯ଼ି...   \n",
       "...                                                  ...   \n",
       "22266  \\nଟ୍ରାକ୍ ସାଇକ୍ଲିଂରେ ମହାସାଗରୀଯ଼ ରେକର୍ଡର ତାଲିକା\\...   \n",
       "22267  \\nଗ୍ରେଟ ଜିମ୍ବାୱେ\\n\\nଗ୍ରେଟ ଜିମ୍ବାୱେ ହେଉଛି ଆଧୁନି...   \n",
       "22268  \\nଆଲବର୍ଟୋ ମୁନୋଜ୍\\n\\nଇସମାଇଲ ମୁନୋଜ ଲୋପେଜ (ଜାନୁଆର...   \n",
       "22269  \\nଏକ ବୈଦିକ ଶବ୍ଦ ସମନ୍ୱଯ଼\\n\\nଏକ ବୈଦିକ ଶବ୍ଦ 'କନକୋ...   \n",
       "22270  \\nଜାସନ୍ ଆଲ୍କିରେ\\n\\nଜାସନ୍ ଆଲ୍କିରେ ଜଣେ ଆମେରିକୀଯ଼...   \n",
       "\n",
       "                                                npi_Deva  \\\n",
       "0      \\nठुलो (बियोन्से गीत)\\n\\n\"ठुलो\" (ठुलो अक्षरमा ...   \n",
       "1      कोरियन। व्युत्पत्ति 2. क्रिया। संयोजन। नोटः जब...   \n",
       "2      \\nह्यारियट सेतो माछा मार्ने\\n\\nह्यारिएट ह्वाइट...   \n",
       "3      \\nआइरिस (पौराणिक कथा)\\n\\nप्राचीन ग्रिक धर्म र ...   \n",
       "4      \\nइओनिस कन्टोयनिस\\n\\nइओनिस कोन्टोयनिस (जन्म जन...   \n",
       "...                                                  ...   \n",
       "22266                                               None   \n",
       "22267  \\nग्रेट जिम्बाब्वे\\n\\nग्रेट जिम्बाब्वे आधुनिक ...   \n",
       "22268  \\nअल्बर्टो मुनोज\\n\\nइस्मेल मुनोज लोपेज (जनवरी ...   \n",
       "22269  \\nवैदिक शब्द समन्वय\\n\\nवैदिक शब्द समन्वय (संस्...   \n",
       "22270  \\nजेसन अल्किरे\\n\\nजेसन अल्किरे एक अमेरिकी फेसन...   \n",
       "\n",
       "                                                pan_Guru  \\\n",
       "0      \\nਵੱਡਾ (ਬੇਓਨਸੇ ਗੀਤ)\\n\\n\"ਵੱਡਾ\" (ਵੱਡੇ ਅੱਖਰ ਵਿੱਚ ...   \n",
       "1      ਕੋਰੀਆਈ. ਸ਼ਬਦ-ਵਿਗਿਆਨ 2. ਕ੍ਰਿਆ। ਸੰਯੋਜਨ. ਨੋਟਃ ਹਾਲ...   \n",
       "2      \\nਹੈਰੀਏਟ ਚਿੱਟਾ ਮਛੇਰਾ\\n\\nਹੈਰੀਏਟ ਵ੍ਹਾਈਟ ਫਿਸ਼ਰ ਐਂ...   \n",
       "3      \\nਆਈਰਿਸ (ਮਿਥਿਹਾਸ)\\n\\nਪ੍ਰਾਚੀਨ ਯੂਨਾਨੀ ਧਰਮ ਅਤੇ ਮਿ...   \n",
       "4      \\nਆਇਓਨਿਸ ਕੋਂਟੋਇਆਨਿਸ\\n\\nਇਓਨਿਸ ਕੋਂਟੋਇਆਨਿਸ (ਜਨਮ ਜ...   \n",
       "...                                                  ...   \n",
       "22266  \\nਟਰੈਕ ਸਾਈਕਲਿੰਗ ਵਿੱਚ ਸਮੁੰਦਰੀ ਰਿਕਾਰਡ ਦੀ ਸੂਚੀ\\n\\...   \n",
       "22267  \\nਮਹਾਨ ਜ਼ਿੰਬਾਬਵੇ\\n\\nਮਹਾਨ ਜ਼ਿੰਬਾਬਵੇ ਆਧੁਨਿਕ ਦੇਸ਼...   \n",
       "22268  \\nਅਲਬਰਟੋ ਮੁਨੋਜ਼\\n\\nਇਸਮਾਇਲ ਮੁਨੋਜ਼ ਲੋਪੇਜ਼ (15 ਜਨ...   \n",
       "22269  \\nਇੱਕ ਵੈਦਿਕ ਸ਼ਬਦ ਸੰਜੋਗ\\n\\nਵੈਦਿਕ ਸ਼ਬਦ ਸੰਜੋਗ (ਸੰ...   \n",
       "22270  \\nਜੈਸਨ ਅਲਕੀਰੇ\\n\\nਜੈਸਨ ਅਲਕੀਰੇ ਇੱਕ ਅਮਰੀਕੀ ਫੈਸ਼ਨ ...   \n",
       "\n",
       "                                                urd_Arab  \n",
       "0      \\nبڑا (بیونسی گانا)\\n\\n\"بڑا\" (بڑے حروف میں اسٹ...  \n",
       "1      کوریائی۔ فعلیات 2. فعل۔ کنجگیشن۔ نوٹ: اگرچہ تج...  \n",
       "2      \\nہیریٹ وائٹ فشر\\n\\nہیریٹ وائٹ فشر اینڈریو (18...  \n",
       "3      \\nآئیرس (افسانے)\\n\\nقدیم یونانی مذہب اور افسان...  \n",
       "4      \\nآئیونیس کونٹوئینس\\n\\nآئیونیس کونٹوئینس (پیدا...  \n",
       "...                                                  ...  \n",
       "22266                                               None  \n",
       "22267  \\nعظیم زمبابوے\\n\\nعظیم زمبابوے جدید ملک زمبابو...  \n",
       "22268  \\nالبرٹو مونوز\\n\\nاسماعیل منوز لوپیز (15 جنوری...  \n",
       "22269  \\nایک ویدک لفظ مطابقت\\n\\nویدک لفظ مطابقت (سنسک...  \n",
       "22270  \\nجیسن الکائر\\n\\nجیسن الکائر ایک امریکی فیشن ڈ...  \n",
       "\n",
       "[22271 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
