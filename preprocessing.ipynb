{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LingoIITGN/ganga-2-1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"token_per_language.json\", 'r') as f:\n",
    "    token_per_language = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitext Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: CrossSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_hindi = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/english-hindi_CrossSum/english-hindi_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        english_hindi.append(json.loads(sample))\n",
    "\n",
    "hindi_english = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/hindi-english_CrossSum/hindi-english_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        hindi_english.append(json.loads(sample))\n",
    "\n",
    "hindi_hindi = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/hindi-hindi_CrossSum/hindi-hindi_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        hindi_hindi.append(json.loads(sample))\n",
    "\n",
    "english_english = []\n",
    "with open(\"./Data/Bitext Mining/CrossSum/english-english_CrossSum/english-english_test.jsonl\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        english_english.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "\n",
    "for sample in english_hindi:\n",
    "    eng = tokenizer.encode(sample['text'])\n",
    "    hin = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_english_tokens+= len(eng)\n",
    "    total_hindi_tokens+= len(hin)\n",
    "\n",
    "token_per_language['english_hindi_crosssum'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "\n",
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in hindi_english:\n",
    "    hin = tokenizer.encode(sample['text'])\n",
    "    eng = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_english_tokens+= len(eng)\n",
    "    total_hindi_tokens+= len(hin)\n",
    "\n",
    "token_per_language['hindi_english_crosssum'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "\n",
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in hindi_hindi:\n",
    "    hin_text = tokenizer.encode(sample['text'])\n",
    "    hin = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_text)\n",
    "    total_hindi_tokens+= len(hin)\n",
    "\n",
    "token_per_language['hindi_hindi_crosssum'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': 0,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "\n",
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in english_english:\n",
    "    english_text = tokenizer.encode(sample['text'])\n",
    "    english = tokenizer.encode(sample['summary'])\n",
    "\n",
    "    total_english_tokens+= len(english_text)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['english_english_crosssum'] = {'Hindi': 0,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_url = []\n",
    "english_url = []\n",
    "\n",
    "hindi_url.extend([sample['target_url'] for sample in english_hindi])\n",
    "hindi_url.extend([sample['target_url'] for sample in hindi_hindi])\n",
    "hindi_url.extend([sample['source_url'] for sample in hindi_hindi])\n",
    "hindi_url.extend([sample['source_url'] for sample in hindi_english])\n",
    "\n",
    "english_url.extend([sample['source_url'] for sample in english_hindi])\n",
    "english_url.extend([sample['target_url'] for sample in english_english])\n",
    "english_url.extend([sample['source_url'] for sample in english_english])\n",
    "english_url.extend([sample['target_url'] for sample in hindi_english])\n",
    "\n",
    "hindi_url = set(hindi_url)\n",
    "english_url = set(english_url)\n",
    "\n",
    "hindi_url_dict = {url: f\"hi_{idx}\" for idx, url in enumerate(hindi_url)}\n",
    "english_url_dict = {url: f\"en_{idx}\" for idx, url in enumerate(english_url)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(english_hindi))\n",
    "print(len(hindi_english))\n",
    "print(len(hindi_hindi))\n",
    "print(len(english_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_english[317]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_hindi_hindi = []\n",
    "summary_instruction = \"निर्देश: दिए गए पाठ के लिए सबसे प्रासंगिक सारांश प्राप्त करें। पाठ:\"\n",
    "query_instruction = \"निर्देश: किसी दिए गए सारांश के लिए सबसे प्रासंगिक पैराग्राफ़ प्राप्त करें। सारांश: \"\n",
    "\n",
    "for idx, sample in enumerate(hindi_hindi):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_hindi_hindi_{hindi_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_hindi_hindi_{hindi_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_hindi_hindi.append(data)\n",
    "\n",
    "with open(\"Processed_data/crosssum_hindi_hindi_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_hindi_hindi:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_english[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_hindi_english = []\n",
    "\n",
    "summary_instruction = \"निर्देश: दिए गए पाठ के लिए सबसे प्रासंगिक सारांश प्राप्त करें। पाठ:\"\n",
    "query_instruction = \"Instructions: Retrieve the most relevant paragraph for a given summary. Summary: \"\n",
    "\n",
    "for idx, sample in enumerate(hindi_english):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_hindi_english_{english_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_hindi_english_{hindi_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_hindi_english.append(data)\n",
    "\n",
    "with open(f\"Processed_data/crosssum_hindi_english_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_hindi_english:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_hindi[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_english_hindi = []\n",
    "\n",
    "summary_instruction = \"Instructions: Retrieve the most relevant summary for the given paragraph. Text: \"\n",
    "query_instruction = \"निर्देश: किसी दिए गए सारांश के लिए सबसे प्रासंगिक पैराग्राफ़ प्राप्त करें। सारांश: \"\n",
    "\n",
    "for idx, sample in enumerate(english_hindi):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_english_hindi_{hindi_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_english_hindi_{english_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_english_hindi.append(data)\n",
    "\n",
    "with open(f\"Processed_data/crosssum_english_hindi_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_english_hindi:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample['text'])) for sample in english_hindi])\n",
    "print(f\"English article length: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['summary'])) for sample in english_hindi])\n",
    "print(f\"Hindi summary length: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['text'])) for sample in hindi_english])\n",
    "print(f\"Hindi article length: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['summary'])) for sample in hindi_english])\n",
    "print(f\"English summary length: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosssum_english_english = []\n",
    "summary_instruction = \"Instructions: Retrieve the most relevant summary from a set of options for the given paragraph. Text: \"\n",
    "query_instruction = \"Instructions: Retrieve the most relevant paragraph from a set of options for a given summary. Summary: \"\n",
    "\n",
    "for idx, sample in enumerate(english_english):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"crosssum_english_english_{english_url_dict[sample['target_url']]}\",\n",
    "                'source': summary_instruction + sample['text'],\n",
    "                'target': sample['summary']}\n",
    "    else:\n",
    "        data = {'id': f\"crosssum_english_english_{english_url_dict[sample['source_url']]}\",\n",
    "                'source': query_instruction + sample['summary'],\n",
    "                'target': sample['text']}\n",
    "\n",
    "\n",
    "    crosssum_english_english.append(data)\n",
    "\n",
    "with open(\"Processed_data/crosssum_english_english_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in crosssum_english_english:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: CrossSum Multi Lingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slang in [\"bengali\", \"burmese\", \"english\" \"gujarati\", \"hindi\", \"marathi\", \"nepali\", \"punjabi\", \"tamil\", \"telugu\", \"urdu\"]:\n",
    "\n",
    "    for tlang in [\"bengali\", \"burmese\", \"english\" \"gujarati\", \"hindi\", \"marathi\", \"nepali\", \"punjabi\", \"tamil\", \"telugu\", \"urdu\"]:\n",
    "\n",
    "        wget f\"https://huggingface.co/datasets/csebuetnlp/CrossSum/resolve/main/data/{slang}-{tlang}_CrossSum.tar.bz2?download=true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Flores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Bitext Mining/Flores/flores_hi_en_test.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['examples'][230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in data['examples']:\n",
    "    hindi = tokenizer.encode(sample['source'])\n",
    "    english = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hindi)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['flores'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flores = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए हिंदी पाठ के लिए अर्थ की दृष्टि से सर्वाधिक समान अंग्रेजी पाठ प्राप्त करें। पाठ: \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar Hindi text for the given English text. Text: \"\n",
    "\n",
    "for idx, sample in enumerate(data['examples']):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data_ = {'id': f\"flores_{idx}\",\n",
    "                'source': hindi_instruction + sample['source'],\n",
    "                'target': sample['target']}\n",
    "    else:\n",
    "        data_ = {'id': f\"flores_{idx}\",\n",
    "                'source': english_instruction + sample['target'],\n",
    "                'target': sample['source']}\n",
    "\n",
    "    flores.append(data_)\n",
    "\n",
    "with open(f\"Processed_data/flores_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in flores:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_texts = np.array([])\n",
    "for sample in data['examples']:\n",
    "    len_of_texts = np.append(len_of_texts, len(tokenizer.encode(sample['source'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_texts.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = []\n",
    "with open(\"./Data/Bitext Mining/LASER/tatoeba.hin-eng.eng\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        english.append(sample)\n",
    "\n",
    "hindi = []\n",
    "with open(\"./Data/Bitext Mining/LASER/tatoeba.hin-eng.hin\", \"r\") as f:\n",
    "    for sample in f:\n",
    "        hindi.append(sample)\n",
    "\n",
    "length = np.array([len(tokenizer.encode(sample)) for sample in english])\n",
    "print(f\"Average length of english sentences: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample)) for sample in hindi])\n",
    "print(f\"Average length of hindi sentences: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(hindi)):\n",
    "    hin = tokenizer.encode(hindi[idx])\n",
    "    eng = tokenizer.encode(english[idx])\n",
    "\n",
    "    total_hindi_tokens+= len(hin)\n",
    "    total_english_tokens+= len(eng)\n",
    "\n",
    "token_per_language['laser'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए हिंदी पाठ के लिए अर्थ की दृष्टि से सर्वाधिक समान अंग्रेजी पाठ प्राप्त करें। पाठ: \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar Hindi text for the given English text. Text: \"\n",
    "\n",
    "for idx, (hin, eng) in enumerate(zip(hindi, english)):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"laser_{idx}\",\n",
    "                'source': hindi_instruction + hin,\n",
    "                'target': eng}\n",
    "    else:\n",
    "        data = {'id': f\"laser_{idx}\",\n",
    "                'source': english_instruction + eng,\n",
    "                'target': hin}\n",
    "\n",
    "    laser.append(data)\n",
    "\n",
    "with open(f\"Processed_data/laser.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    for sample in laser:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Mintaka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Bitext Mining/Mintaka/mintaka_test.json\", 'r') as f:\n",
    "    samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample['question'])) for sample in samples])\n",
    "print(f\"Average length of english questions: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample['translations']['hi'])) for sample in samples])\n",
    "print(f\"Average length of hindi questions: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(samples)):\n",
    "    eng = tokenizer.encode(samples[idx]['question'])\n",
    "    hin = tokenizer.encode(samples[idx]['translations']['hi'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin)\n",
    "    total_english_tokens+= len(eng)\n",
    "\n",
    "token_per_language['Mintaka'] = {'Hindi': total_hindi_tokens,\n",
    "                                                'English': total_english_tokens,\n",
    "                                                'Romanised_Hindi': 0}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mintaka = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: हिंदी प्रश्न के लिए शब्दार्थ की दृष्टि से सर्वाधिक समान अंग्रेजी प्रश्न को पुनः प्राप्त करें। \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar Hindi question for the English question. Question: \"\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"mintaka_{idx}\",\n",
    "                'source': english_instruction + sample['question'],\n",
    "                'target': sample['translations']['hi']}\n",
    "    else:\n",
    "        data = {'id': f\"mintaka_{idx}\",\n",
    "                'source': hindi_instruction + sample['translations']['hi'],\n",
    "                'target': sample['question']}\n",
    "\n",
    "    mintaka.append(data)\n",
    "\n",
    "with open(f\"Processed_data/mintaka_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in mintaka:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: PHINC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/Bitext Mining/PHINC/filtered_data.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample)) for sample in df['Sentence']])\n",
    "print(f\"Average length of english questions: {length.mean()}\")\n",
    "length = np.array([len(tokenizer.encode(sample)) for sample in df['English_Translation']])\n",
    "print(f\"Average length of hindi questions: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[23,'Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[23, 'English_Translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "for sample in df.itertuples():\n",
    "    r_eng = tokenizer.encode(sample.Sentence)\n",
    "    eng = tokenizer.encode(sample.English_Translation)\n",
    "\n",
    "    total_r_english_tokens+= len(r_eng)\n",
    "    total_english_tokens+= len(eng)\n",
    "\n",
    "token_per_language['Mintaka'] = {'Hindi': 0,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phinc = []\n",
    "\n",
    "hindi_instruction = \"nirdesh: die gae romanakrt hindi paath ke lie arth kee drshti se sabase adhik samaan angrejee paath praapt karen. paath: \"\n",
    "english_instruction = \"Instructions: Retrieve the most semantically similar romanized Hindi text for the given English text. Text: \"\n",
    "\n",
    "for idx in range(len(df)):\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"phinc_{idx}\",\n",
    "                'source': hindi_instruction + df.loc[idx, 'Sentence'],\n",
    "                'target': df.loc[idx, 'English_Translation']}\n",
    "    else:\n",
    "        data = {'id': f\"phinc_{idx}\",\n",
    "                'source': english_instruction + df.loc[idx, 'English_Translation'],\n",
    "                'target': df.loc[idx, 'Sentence']}\n",
    "\n",
    "    phinc.append(data)\n",
    "\n",
    "with open(f\"Processed_data/phinc.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in phinc:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: HindiDiscourseClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Classification/HindiDiscourseClassification/discourse_dataset.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(data[key]['Sentence'])) for key in data.keys()])\n",
    "print(f\"Average length of hindi text: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए हिंदी पाठ को निम्नलिखित में से किसी एक श्रेणी में वर्गीकृत करें: 'वर्णनात्मक', 'कथात्मक', 'संवाद', 'तर्कपूर्ण', 'सूचनात्मक', या 'अन्य'। पाठ: \"\n",
    "\n",
    "for key in data.keys():\n",
    "    label = data[key][\"Discourse Mode\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(data[key][\"Sentence\"])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        discourse.append({'id': f'discourse_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(discourse)\n",
    "        \n",
    "with open(f\"Processed_data/discourse.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in discourse:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in discourse:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['discourse'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"./Data/Classification/Massive/hi-IN.jsonl\", 'r') as f:\n",
    "    for sample in f:\n",
    "        data.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "massive = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए आदेश को निम्नलिखित में से किसी एक आशय श्रेणी में वर्गीकृत करें: 'अलार्म', 'ऑडियो', 'आईओटी', 'कैलेंडर', 'प्ले', 'सामान्य', 'डेटटाइम', 'टेकअवे', 'समाचार', 'संगीत', 'मौसम', 'क्यूए', 'सामाजिक', 'सिफारिश', 'खाना पकाना', 'परिवहन', 'ईमेल', 'सूचियाँ'। पाठ: \"\n",
    "\n",
    "for item in data:\n",
    "    label = item[\"scenario\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item[\"utt\"])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        massive.append({'id': f'massive_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(massive)\n",
    "        \n",
    "with open(f\"Processed_data/massive.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in massive:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [sample for sample in data if sample['partition']=='train']\n",
    "length = np.array([len(tokenizer.encode(sample['utt'])) for sample in train_data])\n",
    "print(f\"Average length of hindi text: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in massive:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['massive'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: SentimentAnalysisHindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"./Data/Classification/SentimentAnalysisHindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([len(tokenizer.encode(sample['text'])) for sample in ds['train']])\n",
    "print(f\"Average length of hindi text: {length.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: सकारात्मक, नकारात्मक, या तटस्थ। पाठ: \"\n",
    "\n",
    "for item in ds['train']:\n",
    "    label = item[\"label\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item[\"text\"])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Sentiment Analysis Joshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"./Data/Classification/sent_hineng_joshi/sentiment_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: नकारात्मक (-1), तटस्थ (0), या सकारात्मक (1)। पाठ:\"\n",
    "\n",
    "for (_,item) in data_csv.iterrows():\n",
    "    label = item.iloc[1]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item.iloc[0])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_joshi_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment_joshi.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_r_english_tokens+= len(hin_1)\n",
    "    total_r_english_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment_joshi'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Sentiment Shete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"./Data/Classification/sent_hineng_shete/data.csv\")\n",
    "print(len(data_csv))\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: नकारात्मक (-1), तटस्थ (0), या सकारात्मक (1)। पाठ:\"\n",
    "label_dict = {-1: 'neg',\n",
    "              0: 'neu',\n",
    "              1: 'pos'}\n",
    "for (_,item) in data_csv.iterrows():\n",
    "    label = label_dict[item.iloc[1]]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item.iloc[0])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_shete_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment_shete.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_r_english_tokens+= len(hin_1)\n",
    "    total_r_english_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment_shete'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Sentiment Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"Data/Classification/sent_review/sentiment_reviews.csv\")\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को निम्नलिखित भावना श्रेणियों में से किसी एक में वर्गीकृत करें: नकारात्मक (-1), या सकारात्मक (1)। पाठ:\"\n",
    "\n",
    "for (_,item) in data_csv.iterrows():\n",
    "    label = item.iloc[1]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item.iloc[0])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        sentiment.append({'id': f'sentiment_review_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(sentiment)\n",
    "        \n",
    "with open(f\"Processed_data/sentiment_review.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in sentiment:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in sentiment:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['sentiment_review'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Amazon Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([sample['label'] for sample in ds['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"Instruction: Classify the sentiment of the following Amazon product review into one of the following labels:\\n0 - Very Negative  \\n1 - Negative \\n2 - Neutral  \\n3 - Positive  \\n4 - Very Positive \\nReview: \"\n",
    "\n",
    "for item in ds['train']:\n",
    "    label = item['label']\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item['text'])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        amazon_review.append({'id': f'amazon_review_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(amazon_review)\n",
    "        \n",
    "with open(f\"Processed_data/amazon_review.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in amazon_review:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review_test = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"Instruction: Classify the sentiment of the following Amazon product review into one of the following labels:\\n0 - Very Negative  \\n1 - Negative \\n2 - Neutral  \\n3 - Positive  \\n4 - Very Positive \\nReview: \"\n",
    "\n",
    "for item in ds['validation']:\n",
    "    label = item['label']\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item['text'])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        amazon_review_test.append({'id': f'amazon_review_val{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(amazon_review_test)\n",
    "        \n",
    "with open(f\"Processed_data/amazon_review_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in amazon_review_test:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in amazon_review:\n",
    "    eng_1 = tokenizer.encode(sample['source'])\n",
    "    eng_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_english_tokens+= len(eng_1)\n",
    "    total_english_tokens+= len(eng_2)\n",
    "\n",
    "token_per_language['amazon_review'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ABP news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Data/Classification/ABP_News/ABP_News_classification.json\", \"r\") as f:\n",
    "    abp_news = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp_news_classification = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: निम्नलिखित समाचार लेख को दिए गए श्रेणियों में से किसी एक में वर्गीकृत करें: श्रेणियाँ: gk, technology, business, entertainment, agriculture, astro, lifestyle, sports, education, states. समाचार लेख:\"\n",
    "\n",
    "for key, val in abp_news.items():\n",
    "    label = val['domain']\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(val['article'])\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        abp_news_classification.append({'id': f'abp_news_classification_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(abp_news_classification)\n",
    "        \n",
    "with open(f\"Processed_data/abp_news_classification.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in abp_news_classification:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp_news_classification[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in abp_news_classification:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['abp_news_classification'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: MTOP Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mteb.tasks import MTOPIntentClassification\n",
    "\n",
    "task = MTOPIntentClassification()\n",
    "task.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.dataset['hi']['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "intent = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ को उसके उद्देश्य के आधार पर वर्गीकृत करें। पाठ: \"\n",
    "\n",
    "for item in task.dataset['hi']['train']:\n",
    "    label = item[\"label\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(item[\"text\"])\n",
    "\n",
    "drop_key = []\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    if len(label_groups[key]) < 2:\n",
    "        drop_key.append(key)\n",
    "    \n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    if key not in drop_key:\n",
    "\n",
    "        for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "            pos_idx = list(range(0, len(label_groups[key])))\n",
    "            pos_idx.remove(idx)\n",
    "            intent.append({'id': f'intent_{key}',\n",
    "                            'source': instruction + sent,\n",
    "                            'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(intent)\n",
    "        \n",
    "with open(f\"Processed_data/mtop_intent.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in intent:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in intent:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['intent'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"mteb/xnli\", \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "xnli = []\n",
    "label_groups = {}\n",
    "\n",
    "instruction = \"निर्देश: दिए गए प्रेज़म और हाइपोथेसिस के आधार पर निर्धारित करें कि संबंध 'अनुकूलन (entailment)', 'तटस्थ (neutral)', या 'विरोधाभासी (contradiction)' है। \"\n",
    "\n",
    "for idx in range(392702):\n",
    "    label = ds['train'][idx][\"label\"]\n",
    "    if label not in label_groups:\n",
    "        label_groups[label] = []\n",
    "    label_groups[label].append(f\"आधार: {ds['train'][idx]['premise']} परिकल्पना: {ds['train'][idx]['hypothesis']} \")\n",
    "\n",
    "for key in label_groups.keys():\n",
    "\n",
    "    for idx, sent in enumerate(label_groups[key]):\n",
    "\n",
    "        pos_idx = list(range(0, len(label_groups[key])))\n",
    "        pos_idx.remove(idx)\n",
    "        xnli.append({'id': f'xnli_{key}',\n",
    "                        'source': instruction + sent,\n",
    "                        'target': label_groups[key][random.choice(pos_idx)]})\n",
    "        \n",
    "random.shuffle(xnli)\n",
    "        \n",
    "with open(f\"Processed_data/xnli.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in xnli:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in xnli:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['xnli'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ai4bharat/samanantar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = {}\n",
    "\n",
    "for language in ['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']:\n",
    "    data[language] = load_dataset(\"ai4bharat/samanantar\", language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_as = load_dataset(\"ai4bharat/samanantar\", \"as\")\n",
    "ds_bn = load_dataset(\"ai4bharat/samanantar\", \"bn\")\n",
    "ds_gu = load_dataset(\"ai4bharat/samanantar\", \"gu\")\n",
    "ds_hi = load_dataset(\"ai4bharat/samanantar\", \"hi\")\n",
    "ds_kn = load_dataset(\"ai4bharat/samanantar\", \"kn\")\n",
    "ds_ml = load_dataset(\"ai4bharat/samanantar\", \"ml\")\n",
    "ds_mr = load_dataset(\"ai4bharat/samanantar\", \"mr\")\n",
    "ds_or = load_dataset(\"ai4bharat/samanantar\", \"or\")\n",
    "ds_pa = load_dataset(\"ai4bharat/samanantar\", \"pa\")\n",
    "ds_ta = load_dataset(\"ai4bharat/samanantar\", \"ta\")\n",
    "ds_te = load_dataset(\"ai4bharat/samanantar\", \"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['as']['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "language_classification = []\n",
    "\n",
    "instruction = \"निर्देश: दिए गए पाठ की भाषा को निम्नलिखित भाषाओं में से किसी एक के रूप में वर्गीकृत करें: असमिया (as), बांग्ला (bn), गुजराती (gu), हिंदी (hi), कन्नड़ (kn), मलयालम (ml), मराठी (mr), उड़िया (or), पंजाबी (pa), तमिल (ta), या तेलुगू (te)। पाठ: \"\n",
    "\n",
    "for key in ['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']:\n",
    "\n",
    "    for idx, sent in enumerate(data[key]['train']):\n",
    "\n",
    "        if key=='hi' and idx>5000:\n",
    "            break\n",
    "        elif key!='hi' and idx>500:\n",
    "            break\n",
    "\n",
    "        language_classification.append({'id': f'samanantar_{key}',\n",
    "                        'source': instruction + sent['tgt'],\n",
    "                        'target': data[key]['train'][data[key]['train'].num_rows-1-idx]['tgt']})\n",
    "        \n",
    "random.shuffle(language_classification)\n",
    "        \n",
    "with open(f\"Processed_data/samanantar_language_classification.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in language_classification:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Code Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_csv = pd.read_csv(\"Data/Translation/codemixed_parallel_corpus/English-Hindi code-mixed parallel corpus.csv\")\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in data_csv.itertuples():\n",
    "    hin_r = tokenizer.encode(sample.Sentence)\n",
    "    english = tokenizer.encode(sample.English_Translation)\n",
    "\n",
    "    total_r_english_tokens+= len(hin_r)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['code_mixed'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mixed = []\n",
    "hindi_instruction = \"Nirdesh: Diye gaye Hindi vaakya se sabse saman romanised hindi vaakya dhunde. Vaakya: \"\n",
    "english_instruction = \"Instruction: Find the most similar hindi sentence to the given romanised hindi sentence. Sentence: \"\n",
    "\n",
    "def remove_username(sentence):\n",
    "\n",
    "    return \" \".join([word for word in sentence.split() if word[0]!='@'])\n",
    "\n",
    "for (idx, sample) in data_csv.iterrows():\n",
    "\n",
    "\n",
    "    if idx%2==0:\n",
    "        data = {'id': f\"code_mixed_{idx}\",\n",
    "                'source': hindi_instruction + remove_username(sample.iloc[0]),\n",
    "                'target': remove_username(sample.iloc[1])}\n",
    "    else:\n",
    "        data = {'id': f\"code_mixed_{idx}\",\n",
    "                'source': english_instruction + remove_username(sample.iloc[1]),\n",
    "                'target': remove_username(sample.iloc[0])}\n",
    "\n",
    "\n",
    "    code_mixed.append(data)\n",
    "\n",
    "with open(\"Processed_data/code_mixed.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in code_mixed:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: HinGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data_csv = pd.read_csv(\"Data/Translation/HinGE/HinGE.csv\")\n",
    "data_csv = data_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in data_csv.itertuples():\n",
    "    hindi = tokenizer.encode(sample.Hindi)\n",
    "    english = tokenizer.encode(sample.English)\n",
    "\n",
    "    total_hindi_tokens+= len(hindi)\n",
    "    total_english_tokens+= len(english)\n",
    "\n",
    "token_per_language['hinge'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinge = []\n",
    "englishr_instruction = \"Instruction: From the given English sentence, find the translated romanised Hindi sentence. Sentence: \"\n",
    "hindi_instruction = \"निर्देश: दिए गए हिंदी वाक्य में से अनुवादित रोमनकृत हिंदी वाक्य चुनिए। वाक्य: \"\n",
    "hindir_instruction = \"Nirdesh: Diye gaye romanised hindi vaakya se sabse saman hindi vaakya dhunde. Vaakya: \"\n",
    "hindire_instruction = \"Nirdesh: Diye gaye romanised hindi vaakya se sabse saman english vaakya dhunde. Vaakya: \"\n",
    "\n",
    "\n",
    "for (idx, sample) in data_csv.iterrows():\n",
    "\n",
    "    if len(re.findall(r\"'(.*?)'\", sample.iloc[2]))==0:\n",
    "        continue\n",
    "    if idx%2==0:\n",
    "        data1 = {'id': f\"hinge_{idx}\",\n",
    "                'source': hindi_instruction + sample.iloc[1],\n",
    "                'target': re.findall(r\"'(.*?)'\", sample.iloc[2])[0]}\n",
    "        hinge.append(data1)\n",
    "        data2 = {'id': f\"hinge_{idx}\",\n",
    "                'source': hindir_instruction + re.findall(r\"'(.*?)'\", sample.iloc[2])[0],\n",
    "                'target': sample.iloc[1]}\n",
    "        hinge.append(data2)\n",
    "\n",
    "    else:\n",
    "        data1 = {'id': f\"hinge_{idx}\",\n",
    "                'source': hindire_instruction + re.findall(r\"'(.*?)'\", sample.iloc[2])[0],\n",
    "                'target': sample.iloc[1]}\n",
    "        hinge.append(data1)\n",
    "        data2 = {'id': f\"hinge_{idx}\",\n",
    "                'source': englishr_instruction + sample.iloc[0],\n",
    "                'target': re.findall(r\"'(.*?)'\", sample.iloc[2])[0]}\n",
    "        hinge.append(data2)\n",
    "    \n",
    "\n",
    "\n",
    "    hinge.append(data)\n",
    "\n",
    "with open(\"Processed_data/hinge.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in hinge:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hinge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: IndicQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Retrieval/IndicQA/indicqa.hi.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = set([sample['paragraphs'][0]['context'] for sample in data['data']])\n",
    "context_dict = {context: idx for idx, context in enumerate(context_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][34]['paragraphs'][0]['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicqa = []\n",
    "\n",
    "instruction = \"निर्देश: दिए गए प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक गद्यांश चुनिए। प्रश्न: \"\n",
    "\n",
    "for sample in data['data']:\n",
    "\n",
    "    \n",
    "    context = sample['paragraphs'][0]['context']\n",
    "\n",
    "    for qas in sample['paragraphs'][0]['qas']:\n",
    "\n",
    "        if qas['category'] == 'No':\n",
    "            continue\n",
    "        indicqa.append({\n",
    "            'id': f\"indicqa_{context_dict[context]}\",\n",
    "            'source': instruction + qas['question'],\n",
    "            'target': context\n",
    "        })\n",
    "\n",
    "random.shuffle(indicqa)\n",
    "\n",
    "with open(f\"Processed_data/indicqa.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in indicqa:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in indicqa:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['indicqa'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: MLDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"./Data/Retrieval/MLDR/test.jsonl\", \"r\") as f:\n",
    "\n",
    "    for sample in f:\n",
    "        data.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_passage = []\n",
    "negative_passage = []\n",
    "\n",
    "for sample in data:\n",
    "    for pp in sample['positive_passages']:\n",
    "        positive_passage.append(pp['text'])\n",
    "    for np in sample['negative_passages']:\n",
    "        negative_passage.append(np['text'])\n",
    "\n",
    "positive_passage = set(positive_passage)\n",
    "negative_passage = set(negative_passage)\n",
    "\n",
    "pp_dict = {passage: idx for idx, passage in enumerate(positive_passage)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mldr = []\n",
    "\n",
    "instruction = \"निर्देश: दिए गए प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक अनुच्छेद को चुनें। प्रश्न: \"\n",
    "\n",
    "for sample in data:\n",
    "\n",
    "    passage = sample['positive_passages'][0]['text']\n",
    "    query = sample['query']\n",
    "\n",
    "    mldr.append({\n",
    "        'id': f\"mldir_{pp_dict[passage]}\",\n",
    "        'source': instruction + query,\n",
    "        'target': passage\n",
    "    })\n",
    "\n",
    "random.shuffle(mldr)\n",
    "\n",
    "with open(f\"Processed_data/mldr_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in mldr:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in mldr:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['mldr'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: MLQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/Retrieval/MLQA_V1/test/test-context-en-question-hi.json\", \"r\") as f:\n",
    "    hin_eng_data = json.load(f)\n",
    "with open(\"./Data/Retrieval/MLQA_V1/test/test-context-hi-question-en.json\", \"r\") as f:\n",
    "    eng_hin_data = json.load(f)\n",
    "with open(\"./Data/Retrieval/MLQA_V1/test/test-context-hi-question-hi.json\", \"r\") as f:\n",
    "    hin_hin_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_context = []\n",
    "english_context = []\n",
    "\n",
    "for topic in hin_hin_data['data']:\n",
    "    \n",
    "    for sample in topic['paragraphs']:\n",
    "        hindi_context.append(sample['context'])\n",
    "\n",
    "hindi_context = set(hindi_context)\n",
    "\n",
    "hindi_context_dict = {passage: idx for idx, passage in enumerate(hindi_context)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlqa = []\n",
    "\n",
    "instruction = \"निर्देश: प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक संदर्भ प्राप्त करें। प्रश्न:\"\n",
    "\n",
    "for topic in hin_hin_data['data']:\n",
    "\n",
    "    for sample in topic['paragraphs']:\n",
    "\n",
    "        context = sample['context']\n",
    "\n",
    "        for qas in sample['qas']:\n",
    "            mlqa.append({\n",
    "                'id': f\"mlqa_{hindi_context_dict[context]}\",\n",
    "                'source': instruction + qas['question'],\n",
    "                'target': context               \n",
    "            })\n",
    "\n",
    "random.shuffle(mlqa)\n",
    "with open(\"./Processed_data/mlqa_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in mlqa:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in mlqa:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['mlqa'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ABP news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Data/Retrieval/ABP_news/ABP_new_query_doc.json\", \"r\") as f:\n",
    "    query_doc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp = []\n",
    "\n",
    "instruction = \"निर्देश: प्रश्न के आधार पर उपलब्ध विकल्पों में से सबसे प्रासंगिक समाचार लेख ढूंढें। प्रश्न:\"\n",
    "count = 0\n",
    "\n",
    "for field in query_doc.keys():\n",
    "\n",
    "    for idx, sample in enumerate(query_doc[field]):\n",
    "\n",
    "        context = sample[0]\n",
    "        question = sample[1]\n",
    "\n",
    "        if question is None:\n",
    "            count+=1\n",
    "            continue\n",
    "\n",
    "        abp.append({\n",
    "            'id': f\"abp_{field}_{idx}\",\n",
    "            'source': instruction + question,\n",
    "            'target': context          \n",
    "        })\n",
    "\n",
    "random.shuffle(abp)\n",
    "with open(\"./Processed_data/abp_news.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in abp:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abp[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in abp:\n",
    "    hin_1 = tokenizer.encode(sample['source'])\n",
    "    hin_2 = tokenizer.encode(sample['target'])\n",
    "\n",
    "    total_hindi_tokens+= len(hin_1)\n",
    "    total_hindi_tokens+= len(hin_2)\n",
    "\n",
    "token_per_language['abp'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(ds['train'])):\n",
    "    context = tokenizer.encode(ds['train'][idx]['context'])\n",
    "    qs = tokenizer.encode(ds['train'][idx]['question'])\n",
    "\n",
    "    total_english_tokens+= len(context)\n",
    "    total_english_tokens+= len(qs)\n",
    "\n",
    "token_per_language['squad'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = []\n",
    "\n",
    "instruction = \"Instruction: Given a question, retrieve the most relevant passage. Question: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    context = ds['train'][idx]['context']\n",
    "    question = ds['train'][idx]['question']\n",
    "\n",
    "    squad.append({\n",
    "        'id': f\"squad_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': context          \n",
    "    })\n",
    "\n",
    "random.shuffle(squad)\n",
    "with open(\"./Processed_data/squad.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in squad:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = []\n",
    "\n",
    "instruction = \"Instruction: Given a question, retrieve the most relevant passage. Question: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['validation'])):\n",
    "\n",
    "    context = ds['validation'][idx]['context']\n",
    "    question = ds['validation'][idx]['question']\n",
    "\n",
    "    squad.append({\n",
    "        'id': f\"squad_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': context          \n",
    "    })\n",
    "\n",
    "random.shuffle(squad)\n",
    "with open(\"./Processed_data/squad_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in squad:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sentence-transformers/eli5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for idx in range(len(ds['train'])):\n",
    "    answer = tokenizer.encode(ds['train'][idx]['answer'])\n",
    "    qs = tokenizer.encode(ds['train'][idx]['question'])\n",
    "\n",
    "    total_english_tokens+= len(answer)\n",
    "    total_english_tokens+= len(qs)\n",
    "\n",
    "token_per_language['eli5'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}\n",
    "token_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = []\n",
    "\n",
    "instruction = \"Instruction: Given a question, retrieve the most relevant answer. Question: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    answer = ds['train'][idx]['answer']\n",
    "    question = ds['train'][idx]['question']\n",
    "\n",
    "    eli5.append({\n",
    "        'id': f\"eli5_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(eli5)\n",
    "with open(\"./Processed_data/eli5.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in eli5:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Stackover flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/stackoverflowdupquestions-reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_english_tokens = 0\n",
    "total_r_english_tokens = 0\n",
    "total_hindi_tokens = 0\n",
    "for sample in ds['train']:\n",
    "    query = tokenizer.encode(sample['query'])\n",
    "    positive = tokenizer.encode(sample['positive'][0])\n",
    "\n",
    "    total_english_tokens+= len(query)\n",
    "    total_english_tokens+= len(positive)\n",
    "\n",
    "token_per_language['stackoverflow'] = {'Hindi': total_hindi_tokens,\n",
    "                                'English': total_english_tokens,\n",
    "                                'Romanised_Hindi': total_r_english_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow = []\n",
    "\n",
    "instruction = \"Instruction: Given a query, retrieve the most similar sentence. Query: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    answer = ds['train'][idx]['positive'][0]\n",
    "    question = ds['train'][idx]['query']\n",
    "\n",
    "    stackoverflow.append({\n",
    "        'id': f\"stackoverflow_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(stackoverflow)\n",
    "with open(\"./Processed_data/stackoverflow.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in stackoverflow:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_test = []\n",
    "\n",
    "instruction = \"Instruction: Given a query, retrieve the most similar sentence. Query: \"\n",
    "count = 0\n",
    "\n",
    "for idx in range(len(ds['train'])):\n",
    "\n",
    "    answer = ds['train'][idx]['positive'][0]\n",
    "    question = ds['train'][idx]['query']\n",
    "\n",
    "    stackoverflow_test.append({\n",
    "        'id': f\"stackoverflow_test_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(stackoverflow_test)\n",
    "with open(\"./Processed_data/stackoverflow_test.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in stackoverflow_test:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"token_per_language.json\", 'w') as f:\n",
    "    json.dump(token_per_language, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTEB datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: XNLI New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/xnli\", \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "xnli_new = []\n",
    "\n",
    "instruction = \"Instruction: Given a query, retrieve the most similar sentence. Query: \"\n",
    "count = 0\n",
    "\n",
    "for idx, sample in enumerate(ds['train']):\n",
    "\n",
    "    if sample['label'] != 0:\n",
    "        continue\n",
    "\n",
    "    answer = sample['premise']\n",
    "    question = sample['hypothesis']\n",
    "\n",
    "    xnli_new.append({\n",
    "        'id': f\"xnli_new_{idx}\",\n",
    "        'source': instruction + question,\n",
    "        'target': answer          \n",
    "    })\n",
    "\n",
    "random.shuffle(xnli_new)\n",
    "with open(\"./Processed_data/xnli_new.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in xnli_new:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Belebele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_dev = load_dataset(\"facebook/belebele\", \"hin_Deva\")\n",
    "ds_latin = load_dataset(\"facebook/belebele\", \"hin_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_latin['test'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "belebele = []\n",
    "\n",
    "eng_instruction = \"Instruction: Given a query, retrieve the most similar passage. Query: \"\n",
    "hindi_instruction = \"निर्देश: एक प्रश्न दिया गया है, सबसे समान अनुच्छेद को पुनः प्राप्त करें। प्रश्न: \"\n",
    "count = 0\n",
    "\n",
    "for idx, sample in enumerate(ds_latin['test']):\n",
    "\n",
    "    passage = sample['flores_passage']\n",
    "    question = sample['question']\n",
    "\n",
    "    belebele.append({\n",
    "        'id': f\"belebele_{idx}\",\n",
    "        'source': eng_instruction + question,\n",
    "        'target': passage          \n",
    "    })\n",
    "\n",
    "max_idx = idx\n",
    "\n",
    "for idx, sample in enumerate(ds_dev['test']):\n",
    "\n",
    "    passage = sample['flores_passage']\n",
    "    question = sample['question']\n",
    "\n",
    "    belebele.append({\n",
    "        'id': f\"belebele_{max_idx + idx+1}\",\n",
    "        'source': hindi_instruction + question,\n",
    "        'target': passage          \n",
    "    })\n",
    "\n",
    "random.shuffle(belebele)\n",
    "with open(\"./Processed_data/belebele.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in belebele:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: IN22-Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mteb.tasks import IN22ConvBitextMining\n",
    "\n",
    "task = IN22ConvBitextMining()\n",
    "task.load_data()\n",
    "task.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ai4bharat/IN22-Conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "convbtm = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए पाठ का सबसे समान अनुवाद खोजें। पाठ: \"\n",
    "\n",
    "for idx, sample in enumerate(ds['test']):\n",
    "\n",
    "    for taregt_lang in ['asm_Beng', 'ben_Beng', 'brx_Deva', 'doi_Deva', 'eng_Latn', 'gom_Deva', 'guj_Gujr', 'kan_Knda', 'kas_Arab', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'mni_Mtei', 'npi_Deva', 'ory_Orya', 'pan_Guru', 'san_Deva', 'sat_Olck', 'snd_Deva', 'tam_Taml', 'tel_Telu', 'urd_Arab']:\n",
    "\n",
    "        text = sample['hin_Deva']\n",
    "        target = sample[taregt_lang]\n",
    "\n",
    "        convbtm.append({\n",
    "            'id': f\"convbtm_{idx}\",\n",
    "            'source': hindi_instruction + text,\n",
    "            'target': target          \n",
    "        })\n",
    "\n",
    "random.shuffle(convbtm)\n",
    "with open(\"./Processed_data/convbtm.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in convbtm:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: LinceMTBitextMining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/LinceMTBitextMining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "lince = []\n",
    "\n",
    "instruction = \"Instruction: Find the most similar romanised Hindi sentence of the give english sentence. Sentence: \"\n",
    "\n",
    "for idx, sample in enumerate(ds['test']):\n",
    "\n",
    "\n",
    "    sent1 = sample['hin_Deva']\n",
    "    sent2 = sample[taregt_lang]\n",
    "\n",
    "    lince.append({\n",
    "        'id': f\"lince_{idx}\",\n",
    "        'source': instruction + sent1,\n",
    "        'target': sent2          \n",
    "    })\n",
    "\n",
    "random.shuffle(lince)\n",
    "with open(\"./Processed_data/lince.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in lince:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: WikiReranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ellamind/wikipedia-2023-11-reranking-multilingual\", \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "wikireranking = []\n",
    "\n",
    "hindi_instruction = \"निर्देश: दिए गए प्रश्न के लिए सबसे अधिक समानता रखने वाले दस्तावेज़ को पहचानें और पुनः प्राप्त करें। प्रश्न:\"\n",
    "\n",
    "for idx, sample in enumerate(ds['test']):\n",
    "\n",
    "    query = sample['query']\n",
    "    positive = sample['positive'][0]\n",
    "\n",
    "    wikireranking.append({\n",
    "        'id': f\"wikireranking_{idx}\",\n",
    "        'source': hindi_instruction + query,\n",
    "        'target': positive          \n",
    "    })\n",
    "\n",
    "random.shuffle(wikireranking)\n",
    "with open(\"./Processed_data/wikireranking.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in wikireranking:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: IndicCrosslingualSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mteb.tasks import IndicCrosslingualSTS\n",
    "\n",
    "task = IndicCrosslingualSTS()\n",
    "task.load_data()\n",
    "task.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens per languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hindi = 0\n",
    "English = 0\n",
    "Romanised_Hindi = 0\n",
    "\n",
    "for key in token_per_language.keys():\n",
    "\n",
    "    Hindi+=token_per_language[key]['Hindi']\n",
    "    English+=token_per_language[key]['English']\n",
    "    Romanised_Hindi+=token_per_language[key]['Romanised_Hindi']\n",
    "\n",
    "print(Hindi)\n",
    "print(English)\n",
    "print(Romanised_Hindi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Length Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"./Processed_data/*\")\n",
    "\n",
    "data = []\n",
    "token_len = []\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "\n",
    "        for sample in f:\n",
    "            sample = json.loads(sample)\n",
    "            data.append(sample)\n",
    "\n",
    "            source_tok = len(tokenizer(sample['source']).input_ids)\n",
    "            target_tok = len(tokenizer(sample['target']).input_ids)\n",
    "\n",
    "            token_len.append(source_tok)\n",
    "            token_len.append(target_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "token_len = np.array(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = sorted(token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(token_len, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import glob, json, os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "files = glob.glob(\"./Processed_data/*\")\n",
    "\n",
    "def get_jsonl(file):\n",
    "    data = []\n",
    "    with open(file, 'r') as f:\n",
    "        for sample in f:\n",
    "            data.append(json.loads(sample))\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for sample in data:\n",
    "            json.dump(sample, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    if \"_test\" in file:\n",
    "        continue\n",
    "\n",
    "    if file not in [\"./Processed_data/crosssum_english_hindi.jsonl\", \"./Processed_data/crosssum_hindi_english.jsonl\",\n",
    "                    \"./Processed_data/crosssum_hindi_hindi.jsonl\", \"./Processed_data/crosssum_english_english.jsonl\",\n",
    "                    \"./Processed_data/flores.jsonl\", \"./Processed_data/mintaka.jsonl\",\n",
    "                    \"./Processed_data/mldr.jsonl\", \"./Processed_data/mlqa.jsonl\",\n",
    "                    \"./Processed_data/amazon_review.jsonl\", \"./Processed_data/squad.jsonl\",\n",
    "                    \"./Processed_data/stackoverflow.jsonl\"]:\n",
    "        \n",
    "        data = get_jsonl(file)\n",
    "        random.shuffle(data)\n",
    "        train, val = train_test_split(data, test_size=0.2)\n",
    "\n",
    "        path = f\"./training_data/{file.split('/')[-1].split('.')[0]}/\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        save_jsonl(train, f\"{path}train.jsonl\")\n",
    "        save_jsonl(val, f\"{path}val.jsonl\")\n",
    "    else:\n",
    "        data = get_jsonl(file)\n",
    "        path = f\"./training_data/{file.split('/')[-1].split('.')[0]}/\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        save_jsonl(data, f\"{path}train.jsonl\")\n",
    "\n",
    "        test_path = f\".{file.split('.')[1]}_test.{file.split('.')[2]}\"\n",
    "        data = get_jsonl(test_path)\n",
    "        save_jsonl(data, f\"{path}val.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training_data/laser/train.jsonl\n",
      "./training_data/indicqa/train.jsonl\n",
      "./training_data/crosssum_hindi_hindi/train.jsonl\n",
      "./training_data/flores/train.jsonl\n",
      "./training_data/mldr/train.jsonl\n",
      "./training_data/phinc/train.jsonl\n",
      "./training_data/hinge/train.jsonl\n",
      "./training_data/crosssum_english_english/train.jsonl\n",
      "./training_data/stackoverflow/train.jsonl\n",
      "./training_data/mtop_intent/train.jsonl\n",
      "./training_data/sentiment/train.jsonl\n",
      "./training_data/sentiment_review/train.jsonl\n",
      "./training_data/code_mixed/train.jsonl\n",
      "./training_data/mintaka/train.jsonl\n",
      "./training_data/crosssum_hindi_english/train.jsonl\n",
      "./training_data/xnli_new/train.jsonl\n",
      "./training_data/sentiment_joshi/train.jsonl\n",
      "./training_data/sentiment_shete/train.jsonl\n",
      "./training_data/eli5/train.jsonl\n",
      "./training_data/massive/train.jsonl\n",
      "./training_data/mlqa/train.jsonl\n",
      "./training_data/amazon_review/train.jsonl\n",
      "./training_data/discourse/train.jsonl\n",
      "./training_data/squad/train.jsonl\n",
      "./training_data/crosssum_english_hindi/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "train_files = glob.glob(\"./training_data/*/train.jsonl\", recursive=True)\n",
    "val_files = glob.glob(\"./training_data/*/val.jsonl\", recursive=True)\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "exclude_files = ['sentiment_shete', 'sentiment_joshi', 'hinge', 'code_mixed', 'sentiment_review', 'abp_news', 'crosssum_english_english']\n",
    "exclude_files = ['xnli', 'Wikireranking', 'samanantar_language_classification', 'abp_news_classification', 'abp_news', 'lince', 'convbtm', 'belebele', 'wikireranking']\n",
    "#exclude_files = ['samanantar_language_classification']\n",
    "\n",
    "english_data_files = ['amazon_review', 'crosssum_english_english', 'eli5', 'squad']\n",
    "#english_data_files = []\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        for sample in data:\n",
    "            json.dump(sample, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "for file in train_files:\n",
    "\n",
    "    if file.split('/')[2] in exclude_files:\n",
    "        #print(file)\n",
    "        continue\n",
    "    print(file)\n",
    "\n",
    "    if file.split('/')[2] in english_data_files:\n",
    "        data = []\n",
    "        with open(file, 'r') as f:\n",
    "            for sample in f:\n",
    "                data.append(json.loads(sample))\n",
    "        length_of_data = len(data)\n",
    "        train_data = train_data + data[:length_of_data//2]\n",
    "\n",
    "    else:\n",
    "\n",
    "        with open(file, 'r') as f:\n",
    "            for sample in f:\n",
    "                train_data.append(json.loads(sample))\n",
    "\n",
    "for file in val_files:\n",
    "\n",
    "    if file.split('/')[2] in exclude_files:\n",
    "        continue\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for sample in f:\n",
    "            val_data.append(json.loads(sample))\n",
    "\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "\n",
    "save_jsonl(train_data, \"./new_training_data/train_data_clean.jsonl\")\n",
    "save_jsonl(val_data, \"./new_training_data/val_data_clean.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "659297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Hard Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "def generate_query_for_article(sample):\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=\"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\"\n",
    "    )\n",
    "    \n",
    "    system_message = \"\"\"\n",
    "        You are an AI assistant designed to generate challenging hard negative examples in the same language as the output. Your task is to produce exactly one concise and well-formed hard negative response that seems similar to the correct Output text, but is actually irrelevant for the given Input text. The hard negative should be misleading in a subtle way — close in topic or style, but not a valid answer. Make sure the grammar and vocabulary are correct. Wrap the hard negative inside ## markers like this: ## hard negative text ##*.**\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": f\"Input Text: {sample['source']} \\nOutput Text: {sample['target']} \\nGenerate Hard Negative example: \"}\n",
    "            ],\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            max_tokens=100,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stream=False\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        time.sleep(5)\n",
    "        return generate_query_for_article(sample)\n",
    "    \n",
    "    return completion.choices[0].message.content.split(\"##\")[1]\n",
    "\n",
    "with open(\"./new_training_data/train_data.jsonl\", \"r\") as f:\n",
    "    data = []\n",
    "    for sample in f:\n",
    "        data.append(json.loads(sample))\n",
    "\n",
    "for idx, sample in enumerate(data):\n",
    "    \n",
    "    data[idx]['hard_negative'] = generate_query_for_article(sample)\n",
    "    data[idx]['hard_negative_flag'] = 1\n",
    "\n",
    "    if idx%5000==0:\n",
    "        print(f\"Processed {idx} samples\")\n",
    "        with open(\"./new_training_data/train_data_with_hard_negative.jsonl\", \"w\") as f:\n",
    "            for sample in data:\n",
    "                json.dump(sample, f, ensure_ascii=False)\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LingoIITGN/Ganga-2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bitext_Mining = ['./training_data/crosssum_english_english/train.jsonl',\n",
    "                 './training_data/crosssum_hindi_hindi/train.jsonl',\n",
    "                 './training_data/crosssum_english_hindi/train.jsonl',\n",
    "                 './training_data/crosssum_hindi_english/train.jsonl',\n",
    "                 './training_data/flores/train.jsonl',\n",
    "                 './training_data/laser/train.jsonl',\n",
    "                 './training_data/mintaka/train.jsonl',\n",
    "                 './training_data/phinc/train.jsonl']\n",
    "\n",
    "Classification = ['./training_data/discourse/train.jsonl',\n",
    "                  './training_data/massive/train.jsonl',\n",
    "                  './training_data/sentiment_joshi/train.jsonl',\n",
    "                  './training_data/sentiment_shete/train.jsonl',\n",
    "                  './training_data/sentiment_review/train.jsonl',\n",
    "                  './training_data/sentiment/train.jsonl',\n",
    "                 './training_data/abp_news_classification/train.jsonl',\n",
    "                 './training_data/amazon_review/train.jsonl'\n",
    "                  ]\n",
    "\n",
    "Retrieval = ['./training_data/abp_news/train.jsonl',\n",
    "             './training_data/indicqa/train.jsonl',\n",
    "             './training_data/mldr/train.jsonl',\n",
    "             './training_data/mlqa/train.jsonl',\n",
    "             './training_data/squad/train.jsonl',\n",
    "             './training_data/stackoverflow/train.jsonl',\n",
    "             './training_data/eli5/train.jsonl'\n",
    "            ]\n",
    "\n",
    "Translation = ['./training_data/code_mixed/train.jsonl',\n",
    "               './training_data/hinge/train.jsonl',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {}\n",
    "\n",
    "for task in Bitext_Mining:\n",
    "    task_dict[task] = \"Bitext_Mining\"\n",
    "\n",
    "for task in Classification:\n",
    "    task_dict[task] = \"Classification\"\n",
    "\n",
    "for task in Retrieval:\n",
    "    task_dict[task] = \"Retrieval\"\n",
    "\n",
    "for task in Translation:\n",
    "    task_dict[task] = \"Translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_task = {}\n",
    "for task_type, files in task_dict.items():\n",
    "    for fname in files:\n",
    "        file_to_task[fname] = task_type\n",
    "\n",
    "# Collect all .jsonl files\n",
    "files = glob.glob(\"./training_data/*/train.jsonl\", recursive=True)\n",
    "\n",
    "summary = {\n",
    "    \"overall_total_tokens\": 0,\n",
    "    \"overall_samples\": 0,\n",
    "    \"task_type_summary\": defaultdict(lambda: {\n",
    "        \"total_tokens\": 0,\n",
    "        \"total_samples\": 0,\n",
    "        \"max_token_length\": 0,\n",
    "        \"min_token_length\": float(\"inf\")\n",
    "    }),\n",
    "    \"file_summary\": {}\n",
    "}\n",
    "\n",
    "for file in files:\n",
    "    num_tokens = 0\n",
    "    num_sample = 0\n",
    "    max_length = 0\n",
    "    min_length = float(\"inf\")\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            source = sample['source']\n",
    "            target = sample['target']\n",
    "            token_len = len(tokenizer.encode(source)) + len(tokenizer.encode(target))\n",
    "            \n",
    "            num_tokens += token_len\n",
    "            num_sample += 1\n",
    "            max_length = max(max_length, token_len)\n",
    "            min_length = min(min_length, token_len)\n",
    "        \n",
    "    #filename = os.path.basename(file)\n",
    "    task_type = task_dict.get(file, \"unknown\")\n",
    "\n",
    "    # Update task type summary\n",
    "    task_data = summary[\"task_type_summary\"][task_type]\n",
    "    task_data[\"total_tokens\"] += num_tokens\n",
    "    task_data[\"total_samples\"] += num_sample\n",
    "    task_data[\"max_token_length\"] = max(task_data[\"max_token_length\"], max_length)\n",
    "    task_data[\"min_token_length\"] = min(task_data[\"min_token_length\"], min_length)\n",
    "\n",
    "    summary['file_summary'][file] = {\n",
    "                        \"task_type\": task_type,\n",
    "                        \"total_samples\": num_sample,\n",
    "                        \"total_tokens\": num_tokens,\n",
    "                        \"max_token_length\": max_length,\n",
    "                        \"min_token_length\": min_length,\n",
    "    }\n",
    "\n",
    "    summary[\"overall_total_tokens\"] += num_tokens\n",
    "    summary[\"overall_samples\"] += num_sample\n",
    "\n",
    "# Convert defaultdict to dict for saving\n",
    "summary[\"task_type_summary\"] = dict(summary[\"task_type_summary\"])\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"training_data_stats.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed.utils.zero_to_fp32 import convert_zero_checkpoint_to_fp32_state_dict\n",
    "\n",
    "best_model_dir = \"checkpoints/ganga-2-1b-embeddings-new-equall-finetune-final-3-mean-32-epoch-1/checkpoint-2000\"\n",
    "output_dir = \"checkpoints/ganga-2-1b-embeddings-new-equall-finetune-final-3-mean-32-epoch-1/best_model\"\n",
    "convert_zero_checkpoint_to_fp32_state_dict(best_model_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ganga_modeling import EmbeddingModel\n",
    "from transformers import AutoModel\n",
    "\n",
    "output_dir = \"checkpoints/ganga-2-1b-embeddings-new-equall-finetune-final-mean-32-epoch-1/best_model\"\n",
    "state_dict = torch.load(f\"{output_dir}/pytorch_model.bin\")\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"LingoIITGN/Ganga-2-1B\")\n",
    "model2 = EmbeddingModel(base_model, 'mean')\n",
    "model2.load_state_dict(state_dict)\n",
    "model2.base_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "paths = [\"./Processed_data/mtop_intent.jsonl\",\n",
    "         #\"./Processed_data/samanantar_language_classification.jsonl\",\n",
    "         \"./Processed_data/xnli_new.jsonl\"]\n",
    "\n",
    "finetuning_data = []\n",
    "\n",
    "for path in paths:\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        for sample in f:\n",
    "            finetuning_data.append(json.loads(sample))\n",
    "\n",
    "random.shuffle(finetuning_data)\n",
    "\n",
    "train, test = train_test_split(finetuning_data, test_size=0.1)\n",
    "\n",
    "os.makedirs(\"./new_training_data2\", exist_ok=True)\n",
    "\n",
    "with open(\"./new_training_data2/train_data.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in train:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(\"./new_training_data2/val_data.jsonl\", \"w\") as f:\n",
    "\n",
    "    for sample in test:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./new_training_data/train_data_with_hard_negative.jsonl\", \"r\") as f:\n",
    "    train_data = []\n",
    "    for sample in f:\n",
    "        train_data.append(json.loads(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
